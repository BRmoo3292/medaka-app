from fastapi import FastAPI, UploadFile, HTTPException, Request
from fastapi.responses import FileResponse,Response,StreamingResponse
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
import time
import numpy as np
import asyncio
from collections import defaultdict, deque
import tempfile
import os
import google.generativeai as genai
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®URLã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
DB_URL = os.getenv("DB_URL")
pg_conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
pg_conn.autocommit = True
print(f"[èµ·å‹•æ™‚] DBæ¥ç¶šæˆåŠŸ: {DB_URL}")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)
model_gemini = genai.GenerativeModel(model_name="gemini-2.5-flash")

print(f"[èµ·å‹•æ™‚] DB_URLè¨­å®š: {'ã‚ã‚Š' if DB_URL else 'ãªã—'}")
print(f"[èµ·å‹•æ™‚] OpenAI API: {'è¨­å®šæ¸ˆã¿' if OPENAI_API_KEY else 'æœªè¨­å®š'}")
print(f"[èµ·å‹•æ™‚] Gemini API: {'è¨­å®šæ¸ˆã¿' if GEMINI_API_KEY else 'æœªè¨­å®š'}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
active_session = {}
conversation_history = defaultdict(lambda: deque(maxlen=10))
latest_health = "Normal"
CURRENT_PROFILE_ID = 1
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"]  
)

# Session Poolerå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šé–¢æ•°
def connect_to_database(db_url, max_retries=3):
    if "pooler.supabase.com" in db_url:
        print("[DBæ¥ç¶š] Supabase Pooleræ¥ç¶šã‚’ä½¿ç”¨")
        if ":5432" in db_url:
            print("[DBæ¥ç¶š] Session Pooler (ãƒãƒ¼ãƒˆ5432)")
        elif ":6543" in db_url:
            print("[DBæ¥ç¶š] Transaction Pooler (ãƒãƒ¼ãƒˆ6543)")
        
        if "sslmode=" not in db_url:
            if "?" in db_url:
                db_url += "&sslmode=require"
            else:
                db_url += "?sslmode=require"
    
    print(f"[DBæ¥ç¶š] æ¥ç¶šå…ˆ: {db_url.split('@')[0]}@...")
    
    for attempt in range(max_retries):
        try:
            print(f"[DBæ¥ç¶š] è©¦è¡Œ {attempt + 1}/{max_retries}")
            
            conn = psycopg2.connect(
                db_url,
                cursor_factory=RealDictCursor,
                keepalives=1,
                keepalives_idle=30,
                keepalives_interval=10,
                keepalives_count=5,
                connect_timeout=10
            )
            
            conn.autocommit = True
            
            with conn.cursor() as cur:
                cur.execute("SELECT 1")
                result = cur.fetchone()
                if result:
                    print(f"âœ… DBæ¥ç¶šæˆåŠŸ! (è©¦è¡Œ {attempt + 1})")
                    
                    cur.execute("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        LIMIT 5;
                    """)
                    tables = cur.fetchall()
                    print(f"[DBæƒ…å ±] æ¤œå‡ºã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«: {[t['table_name'] for t in tables]}")
                    
                    return conn
                    
        except psycopg2.OperationalError as e:
            error_msg = str(e)
            print(f"âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {error_msg}")
            
            if "password authentication failed" in error_msg:
                print("[ã‚¨ãƒ©ãƒ¼] ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
                break
            elif "Network is unreachable" in error_msg:
                print("[ã‚¨ãƒ©ãƒ¼] IPv6æ¥ç¶šã®å•é¡Œã§ã™ã€‚Session Poolerã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
                break
            
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"[DBæ¥ç¶š] {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                time.sleep(wait_time)
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            break
    
    return None  
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
try:
    pg_conn = connect_to_database(DB_URL)
    if not pg_conn:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ç¢ºç«‹ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        exit(1)
except Exception as e:
    print(f"âŒ DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
    exit(1)

async def get_profile_async(profile_id: int):
    """éåŒæœŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, get_profile_sync, profile_id)

def get_profile_sync(profile_id: int):
    """åŒæœŸçš„ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
    with pg_conn.cursor() as cur:
        cur.execute("SELECT * FROM profiles WHERE id = %s;", (profile_id,))
        profile = cur.fetchone()
        if not profile:
            raise HTTPException(404, "Profile not found")
        return profile


@app.get("/best.onnx")
async def serve_onnx_model():
    """ãƒ–ãƒ©ã‚¦ã‚¶æ¤œå‡ºç”¨ã®ONNXãƒ¢ãƒ‡ãƒ«ã‚’é…ä¿¡"""
    model_path = "best.onnx"
    if not os.path.exists(model_path):
        raise HTTPException(404, f"Model file not found: {model_path}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(model_path, "rb") as f:
        content = f.read()
    
    # Responseã§ç›´æ¥è¿”ã™ï¼ˆCORSãƒ˜ãƒƒãƒ€ãƒ¼å®Œå…¨åˆ¶å¾¡ï¼‰
    return Response(
        content=content,
        media_type="application/octet-stream",
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, OPTIONS",
            "Access-Control-Allow-Headers": "*",
            "Access-Control-Expose-Headers": "*",
            "Cache-Control": "public, max-age=31536000",
            "Content-Type": "application/octet-stream",
            "Content-Length": str(len(content))
        }
    )

@app.options("/best.onnx")
async def options_onnx_model():
    return Response(
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, OPTIONS",
            "Access-Control-Allow-Headers": "*"
        }
    )


@app.post("/transcribe_audio")
async def transcribe_audio(file: UploadFile):
    """GPT-4o-mini-transcribeã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
    start = time.time()
    audio_content = await file.read()
    with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_audio:
            temp_audio.write(audio_content)
            temp_audio_path = temp_audio.name
    with open(temp_audio_path, "rb") as audio_file:
            transcript = await openai_client.audio.transcriptions.create(
                model="gpt-4o-transcribe",
                file=audio_file,
                language="ja",
                response_format="text"  # ğŸ†• textã«å¤‰æ›´ï¼ˆã‚ˆã‚Šé«˜é€Ÿï¼‰
            )

        # textã®å ´åˆã€transcriptã¯æ–‡å­—åˆ—ã§è¿”ã£ã¦ãã‚‹
    return {
            "text": transcript,  # ç›´æ¥æ–‡å­—åˆ—
            "duration": None,
            "language": "ja"
        }

@app.post("/talk_with_fish_text")
async def talk_with_fish_text(file: UploadFile):
    start_total = time.time()
    time_log = {}
    
    # â±ï¸ 1. éŸ³å£°èªè­˜ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚’å®Œå…¨ä¸¦åˆ—å®Ÿè¡Œ
    t1 = time.time()
    
    # ğŸ”¥ ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
    transcription_task = transcribe_audio(file)
    profile_task = get_profile_async(CURRENT_PROFILE_ID)
    
    # ä¸¡æ–¹ã®å®Œäº†ã‚’å¾…ã¤
    transcription_result, profile = await asyncio.gather(
        transcription_task,
        profile_task
    )
    
    user_input = transcription_result["text"]
    current_stage = profile["development_stage"]
    
    t2 = time.time()
    time_log['01_éŸ³å£°èªè­˜+ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«'] = t2 - t1
    print(f"[â±ï¸ éŸ³å£°èªè­˜+ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸¦åˆ—ï¼‰] {time_log['01_éŸ³å£°èªè­˜+ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«']:.2f}ç§’")
    
    # â±ï¸ 2. ä¼šè©±å±¥æ­´ã®åˆæœŸåŒ–
    t1 = time.time()
    if CURRENT_PROFILE_ID not in conversation_history:
        conversation_history[CURRENT_PROFILE_ID] = []
    current_history = conversation_history[CURRENT_PROFILE_ID]
    session = active_session.get(CURRENT_PROFILE_ID)
    t2 = time.time()
    time_log['02_å±¥æ­´åˆæœŸåŒ–'] = t2 - t1
    print(f"[â±ï¸ å±¥æ­´åˆæœŸåŒ–] {time_log['02_å±¥æ­´åˆæœŸåŒ–']:.2f}ç§’")
    
    assessment_result = None  
    similar_example = None

    if session is None:
        print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 1å›ç›®ã®ä¼šè©± - é¡ä¼¼ä¾‹ã‚’æ¤œç´¢")
        
        # â±ï¸ 3. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
        t1 = time.time()
        similar_example = await find_similar_conversation(user_input, current_stage)
        t2 = time.time()
        time_log['03_ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢'] = t2 - t1
        print(f"[â±ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢] {time_log['03_ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢']:.2f}ç§’")
        
        # â±ï¸ 4. ãƒ¡ãƒ€ã‚«å¿œç­”ç”Ÿæˆ
        t1 = time.time()
        reply_text = get_medaka_reply(user_input, latest_health, current_history, similar_example, profile)
        t2 = time.time()
        time_log['04_å¿œç­”ç”Ÿæˆ'] = t2 - t1
        print(f"[â±ï¸ å¿œç­”ç”Ÿæˆ] {time_log['04_å¿œç­”ç”Ÿæˆ']:.2f}ç§’")
        
        # â±ï¸ 5. ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
        t1 = time.time()
        if (similar_example and 
            'child_reply_1_embedding' in similar_example and 
            similar_example['distance'] < 0.5):
            session = ConversationSession(
                    profile_id=CURRENT_PROFILE_ID,
                    first_input=user_input,
                    medaka_response=reply_text,
                    similar_example=similar_example,
                    current_stage=current_stage
            )
            active_session[CURRENT_PROFILE_ID] = session
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº† - æ¬¡å›åˆ¤å®šå®Ÿè¡Œäºˆå®šï¼ˆé¡ä¼¼åº¦: {similar_example['distance']:.4f}ï¼‰")
        else:
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] é¡ä¼¼åº¦ãŒä½ã„ - é€šå¸¸ã®ä¼šè©±ã¨ã—ã¦å‡¦ç†")
        t2 = time.time()
        time_log['05_ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ'] = t2 - t1
        print(f"[â±ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ] {time_log['05_ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ']:.2f}ç§’")
        
    else:
        print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 2å›ç›®ã®ä¼šè©± - ç™ºé”æ®µéšåˆ¤å®šã‚’å®Ÿè¡Œ")
        
        # â±ï¸ 3. ç™ºé”æ®µéšåˆ¤å®š
        t1 = time.time()
        assessment = await classify_child_response(
            user_input,
            session.similar_example,
            openai_client,
        )
        t2 = time.time()
        time_log['03_ç™ºé”æ®µéšåˆ¤å®š'] = t2 - t1
        print(f"[â±ï¸ ç™ºé”æ®µéšåˆ¤å®š] {time_log['03_ç™ºé”æ®µéšåˆ¤å®š']:.2f}ç§’")
        
        # â±ï¸ 4. åˆ¤å®šçµæœå‡¦ç†
        t1 = time.time()
        assessment_result = {
            'result': assessment[0],
            'maintain_score': round(float(assessment[1]), 3),
            'upgrade_score': round(float(assessment[2]), 3),
            'confidence_score': round(float(abs(assessment[2] - assessment[1])), 5),
            'assessed_at': datetime.now(),
        }
        
        if assessment[0] == "æ˜‡æ ¼":
            new_stage = upgrade_development_stage(CURRENT_PROFILE_ID, current_stage)
            profile["development_stage"] = new_stage
            
            if new_stage != current_stage:
                assessment_result['stage_upgraded'] = True
                assessment_result['previous_stage'] = current_stage
                assessment_result['new_stage'] = new_stage
                print(f"[ä¼šè©±ãƒ•ãƒ­ãƒ¼] ğŸ‰ ç™ºé”æ®µéšãŒæ˜‡æ ¼ã—ã¾ã—ãŸï¼ {current_stage} â†’ {new_stage}")
            else:
                assessment_result['stage_upgraded'] = False
                assessment_result['already_max'] = True
                print(f"[ä¼šè©±ãƒ•ãƒ­ãƒ¼] ã™ã§ã«æœ€é«˜æ®µéš {current_stage} ã«åˆ°é”ã—ã¦ã„ã¾ã™")
        else:
            assessment_result['stage_upgraded'] = False
            print(f"[ä¼šè©±ãƒ•ãƒ­ãƒ¼] ç¾çŠ¶ç¶­æŒ - {current_stage} ã®ã¾ã¾")
        t2 = time.time()
        time_log['04_åˆ¤å®šçµæœå‡¦ç†'] = t2 - t1
        print(f"[â±ï¸ åˆ¤å®šçµæœå‡¦ç†] {time_log['04_åˆ¤å®šçµæœå‡¦ç†']:.2f}ç§’")
        
        # â±ï¸ 5. ãƒ¡ãƒ€ã‚«å¿œç­”ç”Ÿæˆ
        t1 = time.time()
        reply_text = get_medaka_reply(user_input, latest_health, current_history, None, profile)
        t2 = time.time()
        time_log['05_å¿œç­”ç”Ÿæˆ'] = t2 - t1
        print(f"[â±ï¸ å¿œç­”ç”Ÿæˆ] {time_log['05_å¿œç­”ç”Ÿæˆ']:.2f}ç§’")
        
        # â±ï¸ 6. ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†å‡¦ç†
        t1 = time.time()
        session_id = session.complete_session(user_input, assessment)
        del active_session[CURRENT_PROFILE_ID]
        print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] åˆ¤å®šå®Œäº† - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {session_id}")
        t2 = time.time()
        time_log['06_ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†'] = t2 - t1
        print(f"[â±ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†] {time_log['06_ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†']:.2f}ç§’")

    # â±ï¸ 7. ä¼šè©±å±¥æ­´ä¿å­˜
    t1 = time.time()
    conversation_entry = {
            "child": user_input,
            "medaka": reply_text,
            "timestamp": datetime.now(),
            "similar_example_used": similar_example['text'] if similar_example else None,
            "similarity_score": similar_example['distance'] if similar_example else None,
            "has_assessment": assessment_result is not None,
            "assessment_result": assessment_result,
            "session_status": "started" if session and CURRENT_PROFILE_ID in active_session else "completed"
    }
    conversation_history[CURRENT_PROFILE_ID].append(conversation_entry)
    if len(conversation_history[CURRENT_PROFILE_ID]) > 20:
        conversation_history[CURRENT_PROFILE_ID] = conversation_history[CURRENT_PROFILE_ID][-20:]

    print(f"[ä¼šè©±å±¥æ­´] ç¾åœ¨ã®å±¥æ­´ä»¶æ•°: {len(conversation_history[CURRENT_PROFILE_ID])}")
    t2 = time.time()
    time_log['07_å±¥æ­´ä¿å­˜'] = t2 - t1
    print(f"[â±ï¸ å±¥æ­´ä¿å­˜] {time_log['07_å±¥æ­´ä¿å­˜']:.2f}ç§’")
    
    # â±ï¸ 8. TTSæº–å‚™ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹ã¾ã§ï¼‰
    t_stream_start = time.time()
    
    async def audio_stream():
        chunk_count = 0
        t_first_chunk = None
        
        async with openai_client.audio.speech.with_streaming_response.create(
            model="gpt-4o-mini-tts",
            voice="coral",
            instructions="""
            Voice Affect:ã®ã‚“ã³ã‚Šã—ã¦ã„ã¦ã€ã‹ã‚ã„ã‚‰ã—ã„ç„¡é‚ªæ°—ã•  
            Tone:ã»ã‚“ã‚ã‹ã€å°‘ã—ãŠã£ã¨ã‚Šã€è¦ªã—ã¿ã‚„ã™ã„  
            Pacing:å…¨ä½“çš„ã«ã‚†ã£ãã‚Šã‚ã€è¨€è‘‰ã¨è¨€è‘‰ã®é–“ã«ä½™è£•ã‚’æŒãŸã›ã‚‹  
            """,
            speed=1.0,
            input=reply_text,
            response_format="mp3",
        ) as response:
            async for chunk in response.iter_bytes():
                chunk_count += 1
                if chunk_count == 1:
                    t_first_chunk = time.time()
                    first_chunk_time = t_first_chunk - t_stream_start
                    print(f"[â±ï¸ TTSæœ€åˆã®ãƒãƒ£ãƒ³ã‚¯] {first_chunk_time:.2f}ç§’")
                yield chunk
    
    # â±ï¸ ç·å‡¦ç†æ™‚é–“ã®è¨ˆç®—ã¨è¡¨ç¤º
    end_total = time.time()
    total_time = end_total - start_total
    
    print("\n" + "="*50)
    print("â±ï¸  å‡¦ç†æ™‚é–“ã®è©³ç´°")
    print("="*50)
    
    for key in sorted(time_log.keys()):
        duration = time_log[key]
        percentage = (duration / total_time) * 100
        bar_length = int(percentage / 2)
        bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
        print(f"{key:20} â”‚ {bar} â”‚ {duration:6.2f}ç§’ ({percentage:5.1f}%)")
    
    print("-" * 50)
    print(f"{'åˆè¨ˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹ã¾ã§ï¼‰':20} â”‚ {total_time:6.2f}ç§’ (100.0%)")
    print("="*50 + "\n")
    
    return StreamingResponse(
            audio_stream(),
            media_type="audio/mpeg",
            headers={"Content-Disposition": "inline; filename=reply.mp3"}
        )

async def generate_tts(text: str) -> str:
    """TTSç”Ÿæˆï¼ˆéåŒæœŸé–¢æ•°ï¼‰"""
    async with openai_client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",
        voice="coral",
        instructions="""
        Voice Affect:ã®ã‚“ã³ã‚Šã—ã¦ã„ã¦ã€ã‹ã‚ã„ã‚‰ã—ã„ç„¡é‚ªæ°—ã•  
        Tone:ã»ã‚“ã‚ã‹ã€å°‘ã—ãŠã£ã¨ã‚Šã€è¦ªã—ã¿ã‚„ã™ã„  
        Pacing:å…¨ä½“çš„ã«ã‚†ã£ãã‚Šã‚ã€è¨€è‘‰ã¨è¨€è‘‰ã®é–“ã«ä½™è£•ã‚’æŒãŸã›ã‚‹  
        """,
        speed=1.0,
        input=text,
        response_format="mp3",
    ) as response:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tts_file:
            async for chunk in response.iter_bytes():
                tts_file.write(chunk)
            return tts_file.name


# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é–¢æ•°
async def find_similar_conversation(user_input: str, development_stage: str):
    print(f"[ãƒ™ã‚¯ãƒˆãƒ«åŒ–] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}")
    resp = await openai_client.embeddings.create(
        input=[user_input],
        model="text-embedding-ada-002"
    )
    query_vector = resp.data[0].embedding
    print(f"[ãƒ™ã‚¯ãƒˆãƒ«åŒ–]å®Œäº†:(æ¬¡å…ƒ: {len(query_vector)})")
    
    with pg_conn.cursor() as cur:
        cur.execute("""
            SELECT text, fish_text, children_reply_1, children_reply_2,
                   child_reply_1_embedding, child_reply_2_embedding,
                   user_embedding <-> %s::vector as distance
            FROM conversations
            WHERE development_stage = %s
            ORDER BY distance
            LIMIT 1;
        """, (query_vector, development_stage))
        result = cur.fetchone()
        if result:
            print(f"[é¡ä¼¼æ¤œç´¢] è¦‹ã¤ã‹ã£ãŸä¾‹: '{result['text']}'")
            print(f"[é¡ä¼¼æ¤œç´¢] é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {result['distance']:.4f}")
            return result
        else:
            print(f"[é¡ä¼¼æ¤œç´¢] {development_stage}ã«è©²å½“ã™ã‚‹ä¾‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None

def get_medaka_reply(user_input, health_status="ä¸æ˜", conversation_hist=None, similar_example=None, profile_info=None):
    start = time.time()
    
    if health_status == "Active":
        medaka_state = "å…ƒæ°—"
    elif health_status == "Normal":
        medaka_state = "ä¼‘æ†©ä¸­"
    elif health_status == "Lethargic":
        medaka_state = "å…ƒæ°—ãªã„"
    else:
        medaka_state = "ä¼‘æ†©ä¸­"
    
    print("ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹:", medaka_state)
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®å–å¾—
    if profile_info:
        profile_name = profile_info.get('name', 'Unknown')
        age_text = f"{profile_info['age']}æ­³" if profile_info.get('age') else "å¹´é½¢ä¸æ˜"
        stage_text = profile_info.get('development_stage', 'ä¸æ˜')
        profile_context = f"è©±ã—ç›¸æ‰‹: {profile_name}ã•ã‚“ ({age_text}, {stage_text})\n"
        
        # ä¼šè©±å±¥æ­´
        history_context = ""
        if conversation_hist and len(conversation_hist) > 0:
            recent_history = conversation_hist[-3:]
            history_context = "æœ€è¿‘ã®ä¼šè©±å±¥æ­´:\n"
            for i, h in enumerate(recent_history, 1):
                history_context += f"{i}. å…ç«¥ã€Œ{h['child']}ã€â†’ ãƒ¡ãƒ€ã‚«ã€Œ{h['medaka']}ã€\n"
        history_context += "\n"
        
        # ğŸ†• è‡ªå·±è¡¨ç¾ãƒ¬ãƒ™ãƒ«ã®å–å¾—
        stage = profile_info.get('development_stage', 'stage_1')
        
        # stage ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
        if stage == 'stage_1':
            child_expression_level = 1
        elif stage == 'stage_2':
            child_expression_level = 2
        elif stage == 'stage_3':
            child_expression_level = 3
        else:
            child_expression_level = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    else:
        profile_context = ""
        history_context = ""
        child_expression_level = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    # ğŸ†• è‡ªå·±è¡¨ç¾ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸå¿œç­”æˆ¦ç•¥
    if child_expression_level == 1:
        response_strategy = """
ã€å¿œç­”æˆ¦ç•¥ã€‘
å…ç«¥ã®ç™ºè©±ãŒã€ŒæŠ½è±¡çš„ã€ã‹ã€Œå…·ä½“çš„ã€ã‹ã‚’åˆ¤æ–­ã—ã€ä½¿ã„åˆ†ã‘ã¦ãã ã•ã„ã€‚
- **ç™ºè©±ãŒæŠ½è±¡çš„ãªå ´åˆ**: å¿…ãš2æŠã‚„ã€Œã©ã£ã¡ï¼Ÿã€ã§ç­”ãˆã‚’å¼•ãå‡ºã™ã‹ã€å…ç«¥ã®å˜èªã«è¿½åŠ ã®è¨€è‘‰ã‚’ã¤ã‘ã¦èª˜å°ã™ã‚‹ã€‚
- **ç™ºè©±ãŒå…·ä½“çš„ãªå ´åˆ**: å…ç«¥ã®å˜èªã‚’çŸ­æ–‡ã«ç›´ã—ã¦è¿”ã™ã€‚ã¾ãŸã¯ã€ç™ºè©±ã‚’ãã®ã¾ã¾è‚¯å®šã—ã¤ã¤ã€æ„Ÿæƒ…è¡¨ç¾ã‚„èªå½™ã‚’å°‘ã—å¢—ã‚„ã™ï¼ˆä¾‹ï¼šã€Œãã‚Œã„ã€â†’ã€Œãã‚Œã„ã ã­ã€œï¼ãƒ”ã‚«ãƒ”ã‚«ã—ã¦ã¦ã†ã‚Œã—ã„ã­ã€ï¼‰ã€‚
"""
    elif child_expression_level == 2:
        response_strategy = """
ã€å¿œç­”æˆ¦ç•¥ã€‘
å…ç«¥ã®ç™ºè©±ã‚¿ã‚¤ãƒ—ã«åˆã‚ã›ã¦å¯¾å¿œã‚’å¤‰ãˆã¦ãã ã•ã„ã€‚
- **å˜èªã‚„çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã©ã¾ã‚Š**: çŸ­ã„è¿”ç­”ã‚’ç¹°ã‚Šè¿”ã—ãªãŒã‚‰ã€ã€Œã©ã†ã—ã¦ï¼Ÿã€ã€Œã©ã‚“ãªï¼Ÿã€ã€Œä»–ã«ã¯ï¼Ÿã€ã¨è³ªå•ã‚’è¶³ã™ã€‚ã¾ãŸã¯èˆˆå‘³ã«æ²¿ã£ã¦ã€Œã‚‚ã£ã¨è©³ã—ãæ•™ãˆã¦ã€ã¨æ˜ã‚Šä¸‹ã’ã‚‹ã€‚
- **è©±ãŒå˜ç™ºçš„ã§é †åºãŒãªã„**: ã€Œã¾ãšã¯ï¼Ÿã€ã€Œæ¬¡ã¯ï¼Ÿã€ãªã©ã€ç†ç”±ã¥ã‘ã‚„é †åºç«‹ã¦ã‚’ä¿ƒã™ã€‚
- **èªå½™ã‚„æ–‡æ³•ãŒä¸è‡ªç„¶ã§ã€æ–‡è„ˆãŒã‚ºãƒ¬ã¦ã„ã‚‹**: å°‘ã—ã‚ºãƒ¬ãŸèª¬æ˜ã‚„ä¸€æ–¹çš„ãªè©±ã§ã‚‚å¦å®šã›ãšã«èãå½¹ã«ãªã‚‹ã€‚
"""
    else:
        # stage_3 ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        response_strategy = ""
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
    if similar_example:
        prompt = f"""
ã‚ãªãŸã¯æ°´æ§½ã«ä½ã‚€ã‹ã‚ã„ã„ãƒ¡ãƒ€ã‚«ã€Œã‚­ãƒ³ã¡ã‚ƒã‚“ã€ã§ã™ã€‚
ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹: {medaka_state}
{profile_context}
ä»¥ä¸‹ã®ä¾‹ã¨å…¨ãåŒã˜è¨€è‘‰ã§30å­—ç¨‹åº¦ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚
ã€ä¼šè©±ã€‘
å…ç«¥:ã€Œ{similar_example['text']}ã€
ãƒ¡ãƒ€ã‚«:ã€Œ{similar_example['fish_text']}ã€

{history_context}ã€ç¾åœ¨ã®ä¼šè©±ã€‘
å…ç«¥:ã€Œ{user_input}ã€
ãƒ¡ãƒ€ã‚«:
"""
    else:
        # é¡ä¼¼ä¾‹ãŒãªã„å ´åˆã€æˆ¦ç•¥ã‚’çµ„ã¿è¾¼ã‚“ã ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        prompt = f"""
ã‚ãªãŸã¯æ°´æ§½ã«ä½ã‚€ã‹ã‚ã„ã„ãƒ¡ãƒ€ã‚«ã€Œã‚­ãƒ³ã¡ã‚ƒã‚“ã€ã§ã™ã€‚
{profile_context}

{response_strategy}

{history_context}å…ç«¥:ã€Œ{user_input}ã€

ä¸Šè¨˜ã®ã€å¿œç­”æˆ¦ç•¥ã€‘ã«åŸºã¥ãã€30æ–‡å­—ä»¥å†…ã§ã€å„ªã—ãå°å­¦ç”Ÿã‚‰ã—ã„å£èª¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚
ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹: {medaka_state}

ã‚­ãƒ³ã¡ã‚ƒã‚“:"""
    
    print(f"[å¿œç­”ç”Ÿæˆ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆå®Œäº†\n{prompt}")
    
    # Geminiè¨­å®š
    generation_config = genai.types.GenerationConfig(
        temperature=1,
        top_p=0.1,
        top_k=1
    )
    
    response = model_gemini.generate_content(
        prompt,
        generation_config=generation_config
    )
    
    end = time.time()
    reply = response.text.strip()
    
    print(f"[Geminiå¿œç­”ç”Ÿæˆ] æ‰€è¦æ™‚é–“: {end - start:.2f}ç§’")
    print(f"[å¿œç­”ç”Ÿæˆ] ç”Ÿæˆã•ã‚ŒãŸå¿œç­”: '{reply}'")
    
    return reply

class ConversationSession:
    def __init__(self, profile_id: int, first_input: str, medaka_response: str, similar_example: dict, current_stage: str):
        self.profile_id = profile_id
        self.first_child_input = first_input
        self.medaka_response = medaka_response
        self.similar_example = similar_example
        self.current_stage = current_stage
        self.stared_at = datetime.now()

    def complete_session(self, second_input: str, assessment_result: tuple):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Œäº†ã—ã€DBã«ä¿å­˜"""
        self.second_child_input = second_input
        self.assessment_result = assessment_result[0]
        self.maintain_score = round(float(assessment_result[1]), 3)
        self.upgrade_score = round(float(assessment_result[2]), 3)
        self.confidence_score = round(float(abs(self.upgrade_score - self.maintain_score)), 5)
        
        return self._save_to_database()
    
    def _save_to_database(self) -> int:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            with pg_conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO conversation_sessions (
                        profile_id, first_child_input, medaka_response, second_child_input,
                        assessment_result, maintain_similarity_score, upgrade_similarity_score, 
                        confidence_score, current_stage
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s
                    ) RETURNING id;
                """, (
                    self.profile_id,
                    self.first_child_input,
                    self.medaka_response,
                    self.second_child_input,
                    self.assessment_result,
                    self.maintain_score,      
                    self.upgrade_score,       
                    self.confidence_score,   
                    self.current_stage
                ))
                
                session_id = cur.fetchone()['id']
                print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] ä¿å­˜å®Œäº† ID: {session_id}")
                print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] åˆ¤å®šçµæœ: {self.assessment_result} (ä¿¡é ¼åº¦: {self.confidence_score:.3f})")
                
                return session_id
                
        except Exception as e:
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None

STAGE_PROGRESSION = {
    "stage_1": "stage_2",
    "stage_2": "stage_3",
    "stage_3": "stage_3"
}


# ä¼šè©±åˆ†é¡
async def classify_child_response(
        child_response: str,
        similar_conversation: dict,
        openai_client,
        threshold: float = 0.5
) -> tuple[str, float, float]:
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] å…ç«¥ã®å¿œç­”: '{child_response}'")
    
    resp = await openai_client.embeddings.create(
        input=[child_response],
        model="text-embedding-ada-002"
    )
    response_vector = np.array(resp.data[0].embedding)
    
    def convert_to_vector(embedding_data):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        if isinstance(embedding_data, str):
            import json
            return np.array(json.loads(embedding_data), dtype=float)
            
    maintain_vector = convert_to_vector(similar_conversation['child_reply_1_embedding'])
    upgrade_vector = convert_to_vector(similar_conversation['child_reply_2_embedding'])
    
    def cosine_similarity(v1, v2):
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        if len(v1) != len(v2):
            raise ValueError(f"ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒãŒä¸€è‡´ã—ã¾ã›ã‚“: {len(v1)} vs {len(v2)}")
        
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return np.dot(v1, v2) / (norm1 * norm2)
    
    maintain_similarity = cosine_similarity(response_vector, maintain_vector)
    upgrade_similarity = cosine_similarity(response_vector, upgrade_vector)
    
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] ç¾çŠ¶ç¶­æŒã¨ã®é¡ä¼¼åº¦: {maintain_similarity:.4f}")
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] æ˜‡æ ¼ã¨ã®é¡ä¼¼åº¦: {upgrade_similarity:.4f}")
    
    if upgrade_similarity > maintain_similarity and upgrade_similarity > threshold:
        result = "æ˜‡æ ¼"
    else:
        result = "ç¾çŠ¶ç¶­æŒ"
    
    confidence = abs(upgrade_similarity - maintain_similarity)
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] çµæœ: {result} (ä¿¡é ¼åº¦: {confidence:.4f})")
    
    return result, maintain_similarity, upgrade_similarity

#"""ç™ºé”æ®µéšã‚’1ã¤ä¸Šã’ã‚‹"""
def upgrade_development_stage(profile_id: int, current_stage: str) -> str:
    next_stage = STAGE_PROGRESSION.get(current_stage, current_stage)
    
    if next_stage == current_stage:
        print(f"[ç™ºé”æ®µéš] ã™ã§ã«æœ€é«˜æ®µéš: {current_stage}")
        return current_stage
    
    try:
        with pg_conn.cursor() as cur:
            cur.execute("""
                UPDATE profiles 
                SET development_stage = %s,
                    updated_at = NOW()
                WHERE id = %s
                RETURNING development_stage;
            """, (next_stage, profile_id))
            
            result = cur.fetchone()
            
            if result:
                print(f"[ç™ºé”æ®µéš] æ˜‡æ ¼æˆåŠŸ: {current_stage} â†’ {next_stage} (Profile ID: {profile_id})")
                return next_stage
            else:
                print(f"[ç™ºé”æ®µéš] ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: Profile ID {profile_id}")
                return current_stage
                
    except Exception as e:
        print(f"[ç™ºé”æ®µéš] æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return current_stage


# âœ… ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰å…ƒæ°—åº¦ã‚’å—ä¿¡ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/update_health")
async def update_health(request: Request):
    """ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰é€ä¿¡ã•ã‚ŒãŸå…ƒæ°—åº¦ã‚’æ›´æ–°"""
    global latest_health
    
    data = await request.json()
    status = data.get("status", "Unknown")
    avg_speed = data.get("avg_speed", 0)
    score = data.get("score", 0)
    
    latest_health = status
    
    print(f"[å…ƒæ°—åº¦æ›´æ–°] {status} (é€Ÿåº¦: {avg_speed:.2f}px/s, ã‚¹ã‚³ã‚¢: {score})")
    
    return {
        "status": "success",
        "current_health": latest_health
    }

@app.get("/")
async def read_index():
    return FileResponse('index.html', media_type='text/html')

def get_proactive_medaka_message(conversation_count, profile):
    """ä¼šè©±å›æ•°ã«å¿œã˜ã¦ãƒ¡ãƒ€ã‚«ã‹ã‚‰ã®ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    messages = {
        0: ["ã¯ã˜ã‚ã¾ã—ã¦ï¼åƒ•ã€ãã‚“ã¡ã‚ƒã‚“ã ã‚ˆã€œå›ã®åå‰ã¯ãªã‚“ã¦è¨€ã†ã®ï¼Ÿ", "ã‚„ã£ã»ãƒ¼ï¼åƒ•ã¨ãŠè©±ã—ã—ãªã„ï¼Ÿ", "ä»Šæ—¥ã®å›ã¯ã€ã©ã‚“ãªä¸€æ—¥ã ã£ãŸï¼Ÿåƒ•ã¯ã­ã€æ°´è‰ã®ãƒ™ãƒƒãƒ‰ã§ãŠæ˜¼å¯ã—ã¦ãŸã‚“ã ã‚ˆ"],
        1: ["ã“ã‚“ã«ã¡ã¯ï¼ãã‚‡ã†ã¯ä½•ã‚’ã—ã¦éŠã‚“ã ã®ï¼Ÿ ã¼ãã¯ã­ã€æ°´ã®ä¸­ã§ã‚†ã‚‰ã‚†ã‚‰æºã‚Œã‚‹ã®ãŒå¥½ãã ã‚ˆã€‚", "ã²ã¾ã ã‚ˆã€œ!ä¸€ç·’ã«ãŠè©±ã—ã—ã‚ˆï¼", "ä»Šä½•ã—ã¦ã‚‹ã®ï¼Ÿåƒ•ã¯ã­ã€ã®ã‚“ã³ã‚Šæ³³ã„ã§ã‚‹ã‚ˆã€œ"],
        2: ["ã¾ãŸä¼šãˆã¦å¬‰ã—ã„ãªã€œã€ãŠè©±ã—ã—ã‚ˆ", "ã¯ã˜ã‚ã¾ã—ã¦ï¼ã“ã‚Œã‹ã‚‰å›ã¨ã€ã„ãƒ¼ã£ã±ã„ãŠè©±ã—ã—ãŸã„ãªã€‚ã¾ãšã¯ã€å›ã®å¥½ããªã‚‚ã®ã‚’æ•™ãˆã¦ãã‚Œã‚‹ï¼Ÿ", "å›ã®ã“ã¨æ•™ãˆã¦ã»ã—ã„ãªï¼ãŠåå‰ã¯ï¼Ÿ"],
        3: ["ã‚„ã£ã»ãƒ¼ï¼", "ã­ãˆã­ãˆã€èã“ãˆã‚‹ï¼Ÿã‚¬ãƒ©ã‚¹è¶Šã—ã ã‘ã©ã€ã¯ã˜ã‚ã¾ã—ã¦ï¼ã“ã‚Œã‹ã‚‰ã€ã„ãƒ¼ã£ã±ã„ãŠè©±ã—ã—ã‚ˆã†ã­ï¼", "ã“ã‚“ã«ã¡ã¯ï¼åƒ•ã€ãã‚“ã¡ã‚ƒã‚“ã ã‚ˆã€œå›ã®åå‰ã¯ãªã‚“ã¦è¨€ã†ã®ï¼Ÿ"],
        4: ["ä½•ã‹æ°—ã«ãªã‚‹ã“ã¨ã‚ã‚‹ï¼Ÿ", "ä¸€ç·’ã«ãŠè©±ã—ãªã„ï¼Ÿ", "ãŠè©±èã‹ã›ã¦ã€œ"]
    }
    
    stage_key = min(conversation_count, 4)
    
    import random
    return random.choice(messages[stage_key])

@app.post("/check_session_status")
async def check_session_status(request: Request):
    data = await request.json()
    profile_id = data.get("profile_id")
    
    if not profile_id:
        raise HTTPException(400, "profile_id is required")
    
    has_active_session = profile_id in active_session
    medaka_proactive_enabled = os.getenv("MEDAKA_PROACTIVE_ENABLED", "true").lower() == "true"
    
    return {
        "has_active_session": has_active_session,
        "conversation_count": len(conversation_history.get(profile_id, [])),
        "proactive_enabled": medaka_proactive_enabled
    }

@app.post("/get_proactive_message")
async def get_proactive_message(request: Request):
    data = await request.json()
    profile_id = data.get("profile_id")
    
    if not profile_id:
        raise HTTPException(400, "profile_id is required")
    
    with pg_conn.cursor() as cur:
        cur.execute("SELECT * FROM profiles WHERE id = %s;", (profile_id,))
        profile = cur.fetchone()
        if not profile:
            raise HTTPException(404, "Profile not found")
    
    conversation_count = len(conversation_history.get(profile_id, []))
    message = get_proactive_medaka_message(conversation_count, profile)
    
    async with openai_client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",
        voice="coral",
        instructions="""
        Voice Affect:ã®ã‚“ã³ã‚Šã—ã¦ã„ã¦ã€ã‹ã‚ã„ã‚‰ã—ã„ç„¡é‚ªæ°—ã•  
        Tone:ã»ã‚“ã‚ã‹ã€å°‘ã—ãŠã£ã¨ã‚Šã€è¦ªã—ã¿ã‚„ã™ã„  
        Pacing:å…¨ä½“çš„ã«ã‚†ã£ãã‚Šã‚ã€è¨€è‘‰ã¨è¨€è‘‰ã®é–“ã«ä½™è£•ã‚’æŒãŸã›ã‚‹  
        """,
        speed=1.0,
        input=message,
        response_format="mp3",
    ) as response:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tts_file:
            async for chunk in response.iter_bytes():
                tts_file.write(chunk)
            tts_path = tts_file.name
    
    return FileResponse(tts_path, media_type="audio/mpeg", filename="proactive_reply.mp3")

# ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/conversation_history")
async def get_conversation_history():
    """ç¾åœ¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
    if CURRENT_PROFILE_ID in conversation_history:
        return {
            "profile_id": CURRENT_PROFILE_ID,
            "history": list(conversation_history[CURRENT_PROFILE_ID])
        }
    else:
        return {"profile_id": CURRENT_PROFILE_ID, "history": []}

@app.delete("/conversation_history")
async def clear_conversation_history():
    """ç¾åœ¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
    if CURRENT_PROFILE_ID in conversation_history:
        del conversation_history[CURRENT_PROFILE_ID]
    return {"message": f"History cleared for profile {CURRENT_PROFILE_ID}"}

@app.post("/test_vector_search")
async def test_vector_search(request: Request):
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    data = await request.json()
    user_input = data.get("user_input", "")
    stage = data.get("stage", "stage_1")
    
    result = await find_similar_conversation(user_input, stage)
    return {"query": user_input, "stage": stage, "result": result}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)