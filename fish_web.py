from fastapi import FastAPI, UploadFile, HTTPException,Request,Body
from fastapi.responses import StreamingResponse,JSONResponse,FileResponse
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
import time
import numpy as np
from collections import defaultdict, deque
import httpx
import io
import tempfile
import io
import os
import google.generativeai as genai
import csv
from datetime import datetime
import psycopg2# psycopg2ã¯PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from psycopg2.extras import RealDictCursor

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®URLã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®PostgreSQL
DB_URL = os.getenv("DB_URL")
pg_conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
pg_conn.autocommit = True #ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›´ã‚’å³åº§ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åæ˜ ã•ã›ã‚‹ãŸã‚ã«è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆã‚’æœ‰åŠ¹ã«ã™ã‚‹
print(f"[èµ·å‹•æ™‚] DBæ¥ç¶šæˆåŠŸ: {DB_URL}")
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)
INFERENCE_SERVER_URL = os.getenv("INFERENCE_SERVER_URL")
model_gemini = genai.GenerativeModel(model_name="gemini-2.0-flash")
print(f"[èµ·å‹•æ™‚] DB_URLè¨­å®š: {'ã‚ã‚Š' if DB_URL else 'ãªã—'}")
print(f"[èµ·å‹•æ™‚] OpenAI API: {'è¨­å®šæ¸ˆã¿' if OPENAI_API_KEY else 'æœªè¨­å®š'}")
print(f"[èµ·å‹•æ™‚] Gemini API: {'è¨­å®šæ¸ˆã¿' if GEMINI_API_KEY else 'æœªè¨­å®š'}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
active_session ={}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ç”¨
conversation_history = defaultdict(lambda:deque(maxlen=10))  
speed_history = defaultdict(lambda: deque(maxlen=75))
fps = 15
latest_health = "Normal"
track_history = defaultdict(lambda: (0, 0))  
CURRENT_PROFILE_ID = 1  #ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ID
last_similar_example = defaultdict(lambda: None)  # 2å›ç›®ã®ä¼šè©±å¾…ã¡ã®æƒ…å ±ã‚’ä¿æŒ

# Session Poolerå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šé–¢æ•°
def connect_to_database(db_url, max_retries=3):
    #Supabase Session PoolerçµŒç”±ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š
    if "pooler.supabase.com" in db_url:
        print("[DBæ¥ç¶š] Supabase Pooleræ¥ç¶šã‚’ä½¿ç”¨")
        # ãƒãƒ¼ãƒˆç•ªå·ã®ç¢ºèª
        if ":5432" in db_url:
            print("[DBæ¥ç¶š] Session Pooler (ãƒãƒ¼ãƒˆ5432)")
        elif ":6543" in db_url:
            print("[DBæ¥ç¶š] Transaction Pooler (ãƒãƒ¼ãƒˆ6543)")
        
        # SSLãƒ¢ãƒ¼ãƒ‰ã®è¿½åŠ ï¼ˆå¿…é ˆï¼‰
        if "sslmode=" not in db_url:
            if "?" in db_url:
                db_url += "&sslmode=require"
            else:
                db_url += "?sslmode=require"
    
    print(f"[DBæ¥ç¶š] æ¥ç¶šå…ˆ: {db_url.split('@')[0]}@...")
    
    for attempt in range(max_retries):
        try:
            print(f"[DBæ¥ç¶š] è©¦è¡Œ {attempt + 1}/{max_retries}")
            
            # Session Poolerç”¨ã®æ¥ç¶šè¨­å®š
            conn = psycopg2.connect(
                db_url,
                cursor_factory=RealDictCursor,
                keepalives=1,
                keepalives_idle=30,
                keepalives_interval=10,
                keepalives_count=5,
                connect_timeout=10
            )
            
            # autocommitã¯é‡è¦ï¼ˆSession Poolerã§ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã®å•é¡Œã‚’é¿ã‘ã‚‹ï¼‰
            conn.autocommit = True
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            with conn.cursor() as cur:
                cur.execute("SELECT 1")
                result = cur.fetchone()
                if result:
                    print(f"âœ… DBæ¥ç¶šæˆåŠŸ! (è©¦è¡Œ {attempt + 1})")
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª
                    cur.execute("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        LIMIT 5;
                    """)
                    tables = cur.fetchall()
                    print(f"[DBæƒ…å ±] æ¤œå‡ºã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«: {[t['table_name'] for t in tables]}")
                    
                    return conn
                    
        except psycopg2.OperationalError as e:
            error_msg = str(e)
            print(f"âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {error_msg}")
            
            if "password authentication failed" in error_msg:
                print("[ã‚¨ãƒ©ãƒ¼] ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
                print("Supabaseãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‹ã‚‰æ­£ã—ã„æ¥ç¶šæ–‡å­—åˆ—ã‚’å–å¾—ã—ã¦ãã ã•ã„")
                break
            elif "Network is unreachable" in error_msg:
                print("[ã‚¨ãƒ©ãƒ¼] IPv6æ¥ç¶šã®å•é¡Œã§ã™ã€‚Session Poolerã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
                break
            
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"[DBæ¥ç¶š] {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                time.sleep(wait_time)
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            break
    
    return None

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
try:
    pg_conn = connect_to_database(DB_URL)
    if not pg_conn:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ç¢ºç«‹ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        exit(1)
except Exception as e:
    print(f"âŒ DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
    exit(1)



#ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é–¢æ•°
async def find_similar_conversation(user_input: str,development_stage: str):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
         print(f"[ãƒ™ã‚¯ãƒˆãƒ«åŒ–] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}")
         resp = await openai_client.embeddings.create(
            input=[user_input],
            model="text-embedding-ada-002"
        )
         query_vector = resp.data[0].embedding
         print(f"ã€Œãƒ™ã‚¯ãƒˆãƒ«åŒ–]å®Œäº†:(æ¬¡å…ƒ: {len(query_vector)})")
         #é¡ä¼¼æ¤œç´¢
         with pg_conn.cursor() as cur:
             cur.execute("""
                    SELECT text,fish_text,children_reply_1,children_reply_2,
                          child_reply_1_embedding, child_reply_2_embedding,
                         user_embedding <-> %s::vector as distance
                    FROM conversations
                    WHERE development_stage = %s
                    ORDER BY distance
                    LIMIT 1;
                     """, (query_vector, development_stage))
             result = cur.fetchone()
             if result:
                print(f"[é¡ä¼¼æ¤œç´¢] è¦‹ã¤ã‹ã£ãŸä¾‹: '{result['text']}'")
                print(f"[é¡ä¼¼æ¤œç´¢] é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {result['distance']:.4f}")
                return result
             else:
                print(f"[é¡ä¼¼æ¤œç´¢] {development_stage}ã«è©²å½“ã™ã‚‹ä¾‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None

def get_medaka_reply(user_input, healt_status="ä¸æ˜",conversation_hist=None,similar_example=None,profile_info=None):
    start=time.time()
    if healt_status == "Active":
        medaka_state = "å…ƒæ°—"
    elif healt_status == "Normal":
        medaka_state = "ä¼‘æ†©ä¸­"
    elif healt_status == "Lethargic":
        medaka_state = "å…ƒæ°—ãªã„"
    else:
        medaka_state = "ä¼‘æ†©ä¸­"
    print("ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹:",medaka_state)
    if profile_info:
        profile_name = profile_info.get('name', 'Unknown')
        age_text = f"{profile_info['age']}æ­³" if profile_info.get('age') else "å¹´é½¢ä¸æ˜"
        stage_text = profile_info.get('development_stage', 'ä¸æ˜')
        profile_context = f"è©±ã—ç›¸æ‰‹: {profile_name}ã•ã‚“ ({age_text}, {stage_text})\n"
        history_context=""
        if conversation_hist and len(conversation_hist) > 0:
            recent_history = conversation_hist[-3:]
            history_context = "æœ€è¿‘ã®ä¼šè©±å±¥æ­´:\n"
            for i,h in enumerate(recent_history,1):
                history_context += f"{i}. å…ç«¥ã€Œ{h['child']}ã€â†’ ãƒ¡ãƒ€ã‚«ã€Œ{h['medaka']}ã€\n"
        history_context += "\n"
       
    if similar_example:
                # Few-shotå½¢å¼ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = f"""
                ã‚ãªãŸã¯æ°´æ§½ã«ä½ã‚€ã‹ã‚ã„ã„ãƒ¡ãƒ€ã‚«ã€Œã‚­ãƒ³ã¡ã‚ƒã‚“ã€ã§ã™ã€‚
                ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹: {medaka_state}
                {profile_context}
                ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€å…¨ãåŒã˜è¨€è‘‰ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚
                ã€ä¼šè©±ã€‘
                å…ç«¥:ã€Œ{similar_example['text']}ã€
                ãƒ¡ãƒ€ã‚«:ã€Œ{similar_example['fish_text']}ã€

                {history_context}ã€ç¾åœ¨ã®ä¼šè©±ã€‘
                å…ç«¥:ã€Œ{user_input}ã€
                ãƒ¡ãƒ€ã‚«:
                """

    else:
                # é¡ä¼¼ä¾‹ãŒãªã„å ´åˆã®åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                prompt = f"""
        ã‚ãªãŸã¯æ°´æ§½ã«ä½ã‚€ã‹ã‚ã„ã„ãƒ¡ãƒ€ã‚«ã€Œã‚­ãƒ³ã¡ã‚ƒã‚“ã€ã§ã™ã€‚
        ä»¥ä¸‹ã®å¿œç­”æˆ¦ç•¥ã‚’å‚è€ƒã«ã—ã¦
        ãƒ»ç›¸æ‰‹ã®çŸ­ã„è¿”ç­”ã‚’ç¹°ã‚Šè¿”ã—ãªãŒã‚‰ã€ã€Œã©ã†ã—ã¦ï¼Ÿã€ã€Œã©ã‚“ãªï¼Ÿã€ã€Œä»–ã«ã¯ï¼Ÿã€ã¨è³ªå•ã‚’è¶³ã™ã€‚
        ãƒ»ç†ç”±ã¥ã‘ã‚„é †åºç«‹ã¦ã‚’ä¿ƒã™ã€‚ï¼ˆã€Œã¾ãšã¯ï¼Ÿæ¬¡ã¯ï¼Ÿã€ãªã©ï¼‰
        ãƒ»ç›¸æ‰‹ã®èˆˆå‘³ã«æ²¿ã£ã¦ã€Œã‚‚ã£ã¨è©³ã—ãæ•™ãˆã¦ã€ã¨æ˜ã‚Šä¸‹ã’ã‚‹ã€‚
        ãƒ»å°‘ã—ã‚ºãƒ¬ãŸèª¬æ˜ã‚„ä¸€æ–¹çš„ãªè©±ã§ã‚‚ã€å¦å®šã›ãšã«èãå½¹ã«ãªã‚‹ã€‚
        ä¸€å¾€å¾©ã”ã¨ã«ã€Œç¶šãã‚’è©±ã›ã‚‹ãã£ã‹ã‘ã€ã‚’ä¸ãˆã‚‹
        {profile_context}
        30æ–‡å­—ä»¥å†…ã§ã€å„ªã—ãå°å­¦ç”Ÿã‚‰ã—ã„å£èª¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚
        å…ç«¥:ã€Œ{user_input}ã€
        ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹: {medaka_state}
        ã‚­ãƒ³ã¡ã‚ƒã‚“:
        """

    print(f"[å¿œç­”ç”Ÿæˆ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆå®Œäº†",prompt)
    generation_config = genai.types.GenerationConfig(
        temperature=0.5,  # å‰µé€ æ€§ã‚’ä¸‹ã’ã¦ä¸€è²«æ€§ã‚’é‡è¦–
        top_p=0.1,        # ã‚ˆã‚Šæ±ºå®šçš„ãªå¿œç­”
        top_k=1           # æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„é¸æŠè‚¢ã®ã¿
    )
    response = model_gemini.generate_content(
        prompt,
        generation_config=generation_config
    )
    end = time.time()

    reply = response.text.strip()

    print(f"[Geminiå¿œç­”ç”Ÿæˆ] æ‰€è¦æ™‚é–“: {end - start:.2f}ç§’")
    print(f"[å¿œç­”ç”Ÿæˆ] ç”Ÿæˆã•ã‚ŒãŸå¿œç­”: '{reply}'")
    return reply

class ConversationSession:
    def __init__(self,profile_id:int ,first_input:str,medaka_response:str,similar_example: dict, current_stage: str):
        self.profile_id = profile_id
        self.first_child_input = first_input
        self.medaka_response = medaka_response
        self.similar_example = similar_example
        self.current_stage = current_stage
        self.stared_at = datetime.now()

    def complete_session(self, second_input: str, assessment_result: tuple):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Œäº†ã—ã€DBã«ä¿å­˜"""
        self.second_child_input = second_input
        self.assessment_result = assessment_result[0]  # "æ˜‡æ ¼" or "ç¾çŠ¶ç¶­æŒ"
        self.maintain_score = round(float(assessment_result[1]), 3)      # å°æ•°ç¬¬äºŒä½ã¾ã§
        self.upgrade_score = round(float(assessment_result[2]), 3)       # å°æ•°ç¬¬äºŒä½ã¾ã§
        self.confidence_score = round(float(abs(self.upgrade_score - self.maintain_score)), 5)  # å°æ•°ç¬¬äºŒä½ã¾ã§
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        return self._save_to_database()
    
    def _save_to_database(self) -> int:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            with pg_conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO conversation_sessions (
                        profile_id, first_child_input, medaka_response, second_child_input,
                        assessment_result, maintain_similarity_score, upgrade_similarity_score, 
                        confidence_score, current_stage
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s
                    ) RETURNING id;
                """, (
                    self.profile_id,
                    self.first_child_input,
                    self.medaka_response,
                    self.second_child_input,
                    self.assessment_result,
                    self.maintain_score,      
                    self.upgrade_score,       
                    self.confidence_score,   
                    self.current_stage
                ))
                
                session_id = cur.fetchone()['id']
                print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] ä¿å­˜å®Œäº† ID: {session_id}")
                print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] åˆ¤å®šçµæœ: {self.assessment_result} (ä¿¡é ¼åº¦: {self.confidence_score:.3f})")
                
                return session_id
                
        except Exception as e:
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None

#ä¼šè©±åˆ†é¡
async def classify_child_response(
        child_response: str,
        similar_conversation: dict,
        openai_client,
        threshold: float = 0.5
) -> tuple[str, float, float]:
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] å…ç«¥ã®å¿œç­”: '{child_response}'")
    
    # å…ç«¥ã®å¿œç­”ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    resp = await openai_client.embeddings.create(
        input=[child_response],
        model="text-embedding-ada-002"
    )
    response_vector = np.array(resp.data[0].embedding)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ã«å¤‰æ›
    def convert_to_vector(embedding_data):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        if isinstance(embedding_data, str):
            import json
            return np.array(json.loads(embedding_data), dtype=float)
            
    maintain_vector = convert_to_vector(similar_conversation['child_reply_1_embedding'])
    upgrade_vector = convert_to_vector(similar_conversation['child_reply_2_embedding'])
    
    # é¡ä¼¼åº¦è¨ˆç®—
    def cosine_similarity(v1, v2):
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        # ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ãƒã‚§ãƒƒã‚¯
        if len(v1) != len(v2):
            raise ValueError(f"ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒãŒä¸€è‡´ã—ã¾ã›ã‚“: {len(v1)} vs {len(v2)}")
        
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0  # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆã¯0ã‚’è¿”ã™
        
        return np.dot(v1, v2) / (norm1 * norm2)
    
    maintain_similarity = cosine_similarity(response_vector, maintain_vector)
    upgrade_similarity = cosine_similarity(response_vector, upgrade_vector)
    
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] ç¾çŠ¶ç¶­æŒã¨ã®é¡ä¼¼åº¦: {maintain_similarity:.4f}")
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] æ˜‡æ ¼ã¨ã®é¡ä¼¼åº¦: {upgrade_similarity:.4f}")
    
    if upgrade_similarity > maintain_similarity and upgrade_similarity > threshold:
        result = "æ˜‡æ ¼"
    else:
        result = "ç¾çŠ¶ç¶­æŒ"
    
    confidence = abs(upgrade_similarity - maintain_similarity)
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] çµæœ: {result} (ä¿¡é ¼åº¦: {confidence:.4f})")
    
    return result, maintain_similarity, upgrade_similarity

@app.post("/talk_with_fish_text_only")  # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’è¿”ã™ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
async def talk_with_fish_text_only(request: Request):
    """ãƒ¡ãƒ€ã‚«ã¨ã®ä¼šè©±ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã¿ï¼‰"""
    start_total = time.time()
    data = await request.json()
    user_input = data.get("user_input", "")
    session_id = data.get("session_id", "")  # ä»Šå›ã¯ä½¿ç”¨ã—ãªã„ãŒã€äº’æ›æ€§ã®ãŸã‚å—ã‘å–ã‚‹
    
    if not user_input.strip():
        raise HTTPException(400, "user_input is required")
    
    # 1) ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¸å–å¾—
    with pg_conn.cursor() as cur:
        cur.execute("SELECT * FROM profiles WHERE id = %s;", (CURRENT_PROFILE_ID,))
        profile = cur.fetchone()
        if not profile:
            raise HTTPException(404, "Profile not found")
        current_stage = profile["development_stage"]
    
    # ä¼šè©±å±¥æ­´ã®å–å¾—/åˆæœŸåŒ–
    if CURRENT_PROFILE_ID not in conversation_history:
        conversation_history[CURRENT_PROFILE_ID] = []
    current_history = conversation_history[CURRENT_PROFILE_ID]
    
    session = active_session.get(CURRENT_PROFILE_ID)
    assessment_result = None
    similar_example = None

    if session is None:
        # 1å›ç›®ã®ä¼šè©±ï¼šé¡ä¼¼ä¾‹ã‚’æ¤œç´¢
        print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 1å›ç›®ã®ä¼šè©± - é¡ä¼¼ä¾‹ã‚’æ¤œç´¢")
        similar_example = await find_similar_conversation(user_input, current_stage)
        # é¡ä¼¼ä¾‹ã‚’ä¿å­˜ï¼ˆæ¬¡å›ã®åˆ¤å®šç”¨ï¼‰
        if (similar_example and 
            'child_reply_1_embedding' in similar_example and 
            similar_example['distance'] < 0.5):
            reply_text = get_medaka_reply(user_input, latest_health, current_history, similar_example, profile)
            session = ConversationSession(
                profile_id=CURRENT_PROFILE_ID,
                first_input=user_input,
                medaka_response=reply_text,
                similar_example=similar_example,
                current_stage=current_stage
            )
            active_session[CURRENT_PROFILE_ID] = session
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº† - æ¬¡å›åˆ¤å®šå®Ÿè¡Œäºˆå®šï¼ˆé¡ä¼¼åº¦: {similar_example['distance']:.4f}ï¼‰")
        else:
            reply_text = get_medaka_reply(user_input, latest_health, current_history, None, profile)
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] é¡ä¼¼åº¦ãŒä½ã„ - é€šå¸¸ã®ä¼šè©±ã¨ã—ã¦å‡¦ç†")
    else:
        # 2å›ç›®ã®ä¼šè©±ã®å ´åˆã€ç™ºé”æ®µéšåˆ¤å®šã‚’å®Ÿè¡Œ
        print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 2å›ç›®ã®ä¼šè©± - ç™ºé”æ®µéšåˆ¤å®šã‚’å®Ÿè¡Œ")
        
        # å…ç«¥ã®å¿œç­”åˆ†é¡
        assessment = await classify_child_response(
            user_input,
            session.similar_example,
            openai_client,
        )
        
        assessment_result = {
            'result': assessment[0],
            'maintain_score': round(float(assessment[1]), 3),
            'upgrade_score': round(float(assessment[2]), 3),
            'confidence_score': round(float(abs(assessment[2] - assessment[1])), 5),
            'assessed_at': datetime.now().isoformat(),
        }
        
        reply_text = get_medaka_reply(user_input, latest_health, current_history, None, profile)
        session_id = session.complete_session(user_input, assessment)
        del active_session[CURRENT_PROFILE_ID]  # ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†å¾Œã¯å‰Šé™¤
        print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] åˆ¤å®šå®Œäº† - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {session_id}")

    # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
    conversation_entry = {
        "child": user_input,
        "medaka": reply_text,
        "timestamp": datetime.now().isoformat(),
        "similar_example_used": similar_example['text'] if similar_example else None,
        "similarity_score": similar_example['distance'] if similar_example else None,
        "has_assessment": assessment_result is not None,
        "assessment_result": assessment_result,
        "session_status": "started" if session and CURRENT_PROFILE_ID in active_session else "completed"
    }
    
    conversation_history[CURRENT_PROFILE_ID].append(conversation_entry)
    if len(conversation_history[CURRENT_PROFILE_ID]) > 20:
        conversation_history[CURRENT_PROFILE_ID] = conversation_history[CURRENT_PROFILE_ID][-20:]

    print(f"[ä¼šè©±å±¥æ­´] ç¾åœ¨ã®å±¥æ­´ä»¶æ•°: {len(conversation_history[CURRENT_PROFILE_ID])}")
    
    end_total = time.time()
    print(f"[ç·å‡¦ç†æ™‚é–“] {end_total - start_total:.2f}ç§’")
    
    # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
    return {
        "reply": reply_text,
        "assessment_result": assessment_result,
        "session_status": "started" if CURRENT_PROFILE_ID in active_session else "completed",
        "processing_time": round(end_total - start_total, 2)
    }

@app.post("/predict")
async def predict(file: UploadFile):
   global latest_health
   # å…¨ä½“ã®é–‹å§‹æ™‚é–“
   total_start = time.time()
   timestamps = {}
   try:
       # 1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ™‚é–“æ¸¬å®š
       file_read_start = time.time()
       file_content = await file.read()
       timestamps['file_read'] = (time.time() - file_read_start) * 1000
       
       # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±
       file_size_kb = len(file_content) / 1024
       print(f"ğŸ“ File size: {file_size_kb:.1f}KB")
       
       # 2. HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆæ™‚é–“
       client_create_start = time.time()
       client = httpx.AsyncClient(timeout=30.0)
       timestamps['client_create'] = (time.time() - client_create_start) * 1000
       
       # 3. ãƒªã‚¯ã‚¨ã‚¹ãƒˆæº–å‚™æ™‚é–“
       request_prep_start = time.time()
       files = {"file": (file.filename, io.BytesIO(file_content), file.content_type)}
       timestamps['request_prep'] = (time.time() - request_prep_start) * 1000
       
       # 4. HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡æ™‚é–“ï¼ˆæœ€é‡è¦ï¼‰
       http_request_start = time.time()
       response = await client.post(f"{INFERENCE_SERVER_URL}/predict", files=files)
       timestamps['http_request'] = (time.time() - http_request_start) * 1000
       
       # 5. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¤œè¨¼æ™‚é–“
       validation_start = time.time()
       if response.status_code != 200:
           await client.aclose()
           raise HTTPException(status_code=response.status_code, 
                             detail=f"Inference server error: {response.text}")
       timestamps['validation'] = (time.time() - validation_start) * 1000
       
       # 6. ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ™‚é–“
       header_start = time.time()
       health_status = response.headers.get("X-Health-Status", "Unknown")
       latest_health = health_status
       timestamps['header_process'] = (time.time() - header_start) * 1000
       
       # 7. ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†æ™‚é–“
       response_prep_start = time.time()
       response_content = response.content
       response_size_kb = len(response_content) / 1024
       result = StreamingResponse(
           io.BytesIO(response_content),
           media_type="image/jpeg"
       )
       timestamps['response_prep'] = (time.time() - response_prep_start) * 1000
       
       # 8. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ™‚é–“
       cleanup_start = time.time()
       await client.aclose()
       timestamps['cleanup'] = (time.time() - cleanup_start) * 1000
       
       # åˆè¨ˆæ™‚é–“è¨ˆç®—
       total_time = (time.time() - total_start) * 1000
       
       # ğŸ” è©³ç´°ãƒ­ã‚°å‡ºåŠ›
       print("\n" + "="*50)
       print("ğŸš€ PERFORMANCE ANALYSIS")
       print("="*50)
       print(f"ğŸ“¤ Upload size: {file_size_kb:.1f}KB")
       print(f"ğŸ“¥ Response size: {response_size_kb:.1f}KB")
       print(f"ğŸŒ Total data: {(file_size_kb + response_size_kb):.1f}KB")
       print("-" * 30)
       
       for step, duration in timestamps.items():
           percentage = (duration / total_time) * 100
           bar_length = int(percentage / 5)  # 5%ã«ã¤ã1æ–‡å­—
           bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
           print(f"{step:15} â”‚ {bar} â”‚ {duration:6.1f}ms ({percentage:4.1f}%)")
       
       print("-" * 30)
       print(f"â±ï¸  TOTAL TIME: {total_time:.1f}ms")
       
       # ğŸš¨ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
       max_step = max(timestamps.items(), key=lambda x: x[1])
       if max_step[1] > total_time * 0.5:  # 50%ä»¥ä¸Šã‚’å ã‚ã‚‹å‡¦ç†
           print(f"ğŸš¨ BOTTLENECK: {max_step[0]} ({max_step[1]:.1f}ms)")
       
       # ğŸ“Š é€šä¿¡é€Ÿåº¦è¨ˆç®—
       if timestamps['http_request'] > 0:
           total_data_mb = (file_size_kb + response_size_kb) / 1024
           speed_mbps = (total_data_mb * 8) / (timestamps['http_request'] / 1000)
           print(f"ğŸŒ Effective speed: {speed_mbps:.2f} Mbps")
           
           # é€Ÿåº¦åˆ¤å®š
           if speed_mbps < 1:
               print("ğŸŒ Very slow - likely bandwidth limited")
           elif speed_mbps < 10:
               print("âš ï¸  Slow - network/tunnel overhead")
           else:
               print("âœ… Good speed - latency is the issue")
       
       print("="*50 + "\n")
       
       return result
           
   except httpx.TimeoutException:
       print(f"â° Timeout after {(time.time() - total_start)*1000:.1f}ms")
       raise HTTPException(status_code=504, detail="Inference server timeout")
   except httpx.ConnectError:
       print(f"ğŸ”Œ Connection error after {(time.time() - total_start)*1000:.1f}ms")
       raise HTTPException(status_code=503, detail="Cannot connect to inference server")
   except Exception as e:
       print(f"ğŸ’¥ Error after {(time.time() - total_start)*1000:.1f}ms: {str(e)}")
       raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")
   
@app.get("/")
async def read_index():
    return FileResponse('index.html', media_type='text/html')

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
def get_proactive_medaka_message(conversation_count, profile):
    """ä¼šè©±å›æ•°ã«å¿œã˜ã¦ãƒ¡ãƒ€ã‚«ã‹ã‚‰ã®ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    
    # å…¨ã‚¹ãƒ†ãƒ¼ã‚¸å…±é€šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ã‚¿ãƒ¼ãƒ³
    messages = {
        0: ["ã¯ã˜ã‚ã¾ã—ã¦ï¼åƒ•ã€ãã‚“ã¡ã‚ƒã‚“ã ã‚ˆã€œå›ã®åå‰ã¯ãªã‚“ã¦è¨€ã†ã®ï¼Ÿ", "ã‚„ã£ã»ãƒ¼ï¼åƒ•ã¨ãŠè©±ã—ã—ãªã„ï¼Ÿ", "ä»Šæ—¥ã®å›ã¯ã€ã©ã‚“ãªä¸€æ—¥ã ã£ãŸï¼Ÿåƒ•ã¯ã­ã€æ°´è‰ã®ãƒ™ãƒƒãƒ‰ã§ãŠæ˜¼å¯ã—ã¦ãŸã‚“ã ã‚ˆ"],
        1: ["ã“ã‚“ã«ã¡ã¯ï¼ãã‚‡ã†ã¯ä½•ã‚’ã—ã¦éŠã‚“ã ã®ï¼Ÿ ã¼ãã¯ã­ã€æ°´ã®ä¸­ã§ã‚†ã‚‰ã‚†ã‚‰æºã‚Œã‚‹ã®ãŒå¥½ãã ã‚ˆã€‚", "ã²ã¾ã ã‚ˆã€œ!ä¸€ç·’ã«ãŠè©±ã—ã—ã‚ˆï¼","ä»Šä½•ã—ã¦ã‚‹ã®ï¼Ÿåƒ•ã¯ã­ã€ã®ã‚“ã³ã‚Šæ³³ã„ã§ã‚‹ã‚ˆã€œ"],
        2: ["ã¾ãŸä¼šãˆã¦å¬‰ã—ã„ãªã€œã€ãŠè©±ã—ã—ã‚ˆ", "ã¯ã˜ã‚ã¾ã—ã¦ï¼ã“ã‚Œã‹ã‚‰å›ã¨ã€ã„ãƒ¼ã£ã±ã„ãŠè©±ã—ã—ãŸã„ãªã€‚ã¾ãšã¯ã€å›ã®å¥½ããªã‚‚ã®ã‚’æ•™ãˆã¦ãã‚Œã‚‹ï¼Ÿ","å›ã®ã“ã¨æ•™ãˆã¦ã»ã—ã„ãªï¼ãŠåå‰ã¯ï¼Ÿ"],
        3: ["ã‚„ã£ã»ãƒ¼ï¼", "ã­ãˆã­ãˆã€èã“ãˆã‚‹ï¼Ÿã‚¬ãƒ©ã‚¹è¶Šã—ã ã‘ã©ã€ã¯ã˜ã‚ã¾ã—ã¦ï¼ã“ã‚Œã‹ã‚‰ã€ã„ãƒ¼ã£ã±ã„ãŠè©±ã—ã—ã‚ˆã†ã­ï¼", "ã“ã‚“ã«ã¡ã¯ï¼åƒ•ã€ãã‚“ã¡ã‚ƒã‚“ã ã‚ˆã€œå›ã®åå‰ã¯ãªã‚“ã¦è¨€ã†ã®ï¼Ÿ"],
        4: ["ä½•ã‹æ°—ã«ãªã‚‹ã“ã¨ã‚ã‚‹ï¼Ÿ", "ä¸€ç·’ã«ãŠè©±ã—ãªã„ï¼Ÿ", "ãŠè©±èã‹ã›ã¦ã€œ"]
    }
    
    # ä¼šè©±å›æ•°ã«å¿œã˜ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é¸æŠï¼ˆæœ€å¤§4ã¾ã§ï¼‰
    stage_key = min(conversation_count, 4)
    
    import random
    return random.choice(messages[stage_key])    
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ç¢ºèªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/check_session_status")
async def check_session_status(request: Request):
    data = await request.json()
    profile_id = data.get("profile_id")
    
    if not profile_id:
        raise HTTPException(400, "profile_id is required")
    
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    has_active_session = profile_id in active_session
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’å–å¾—
    medaka_proactive_enabled = os.getenv("MEDAKA_PROACTIVE_ENABLED", "true").lower() == "true"
    
    return {
        "has_active_session": has_active_session,
        "conversation_count": len(conversation_history.get(profile_id, [])),
        "proactive_enabled": medaka_proactive_enabled
    }
# ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/get_proactive_message")
async def get_proactive_message(request: Request):
    data = await request.json()
    profile_id = data.get("profile_id")
    
    if not profile_id:
        raise HTTPException(400, "profile_id is required")
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
    with pg_conn.cursor() as cur:
        cur.execute("SELECT * FROM profiles WHERE id = %s;", (profile_id,))
        profile = cur.fetchone()
        if not profile:
            raise HTTPException(404, "Profile not found")
    
    # ä¼šè©±å›æ•°ã‚’å–å¾—
    conversation_count = len(conversation_history.get(profile_id, []))
    
    # ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
    message = get_proactive_medaka_message(conversation_count, profile)
    
    # â˜… ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ï¼ˆéŸ³å£°åˆæˆãªã—ï¼‰
    return JSONResponse(content={
        "message": message,
        "conversation_count": conversation_count,
        "profile_name": profile.get('name', 'Unknown'),
        "development_stage": profile.get('development_stage', 'ä¸æ˜')
    })


#--------ãƒ‡ãƒãƒƒã‚¯ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ--------
@app.get("/conversation_history")
async def get_conversation_history():
    """ç¾åœ¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
    if CURRENT_PROFILE_ID in conversation_history:
        return {
            "profile_id": CURRENT_PROFILE_ID,
            "history": conversation_history[CURRENT_PROFILE_ID]
        }
    else:
        return {"profile_id": CURRENT_PROFILE_ID, "history": []}

@app.delete("/conversation_history")
async def clear_conversation_history():
    """ç¾åœ¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
    if CURRENT_PROFILE_ID in conversation_history:
        del conversation_history[CURRENT_PROFILE_ID]
    return {"message": f"History cleared for profile {CURRENT_PROFILE_ID}"}

@app.post("/test_vector_search")
async def test_vector_search(request: Request):
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    data = await request.json()
    user_input = data.get("user_input", "")
    stage = data.get("stage", "stage_1")
    
    result = await find_similar_conversation(user_input, stage)
    return {"query": user_input, "stage": stage, "result": result}
#-----------------------------------
