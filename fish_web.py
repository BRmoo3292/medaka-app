import os
from dotenv import load_dotenv

# æœ€åˆã«ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

from fastapi import FastAPI, UploadFile, HTTPException, Request
from fastapi.responses import StreamingResponse, FileResponse
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
import time
import numpy as np
from collections import defaultdict, deque
import httpx
import io
import tempfile
import csv
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor

# ç’°å¢ƒå¤‰æ•°å–å¾—
DB_URL = os.getenv("DB_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
INFERENCE_SERVER_URL = os.getenv("INFERENCE_SERVER_URL", "http://localhost:8001")

print(f"[èµ·å‹•æ™‚] DB_URLè¨­å®š: {'ã‚ã‚Š' if DB_URL else 'ãªã—'}")
print(f"[èµ·å‹•æ™‚] OpenAI API: {'è¨­å®šæ¸ˆã¿' if OPENAI_API_KEY else 'æœªè¨­å®š'}")
print(f"[èµ·å‹•æ™‚] Gemini API: {'è¨­å®šæ¸ˆã¿' if GEMINI_API_KEY else 'æœªè¨­å®š'}")

if not DB_URL:
    print("âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: DB_URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    exit(1)

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
try:
    pg_conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
    pg_conn.autocommit = True
    print(f"âœ… DBæ¥ç¶šæˆåŠŸ!")
except Exception as e:
    print(f"âŒ DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
    exit(1)

# OpenAIè¨­å®šï¼ˆæ¡ä»¶ä»˜ãï¼‰
openai_client = None
if OPENAI_API_KEY and OPENAI_API_KEY.startswith("sk-"):
    try:
        from openai import AsyncOpenAI
        openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        print("âœ… OpenAI APIè¨­å®šå®Œäº†")
    except Exception as e:
        print(f"âš ï¸ OpenAIè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        openai_client = None
else:
    print("âš ï¸ OpenAI APIæœªè¨­å®š")

# Geminiè¨­å®šï¼ˆæ¡ä»¶ä»˜ãï¼‰
model_gemini = None
if GEMINI_API_KEY and GEMINI_API_KEY.startswith("AIza"):
    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        model_gemini = genai.GenerativeModel(model_name="gemini-2.0-flash")
        print("âœ… Gemini APIè¨­å®šå®Œäº†")
    except Exception as e:
        print(f"âš ï¸ Geminiè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        model_gemini = None
else:
    print("âš ï¸ Gemini APIæœªè¨­å®š")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
CONVERSATION_LOG_FILE = "conversation_log.csv"
conversation_history = defaultdict(lambda: deque(maxlen=10))
speed_history = defaultdict(lambda: deque(maxlen=75))
fps = 15
latest_health = "Normal"
track_history = defaultdict(lambda: (0, 0))
CURRENT_PROFILE_ID = 1
last_similar_example = defaultdict(lambda: None)

# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é–¢æ•°
async def find_similar_conversation(user_input: str, development_stage: str):
    if not openai_client:
        print("[ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢] OpenAIæœªè¨­å®šã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢")
        try:
            with pg_conn.cursor() as cur:
                cur.execute("""
                    SELECT text, fish_text, children_reply_1, children_reply_2,
                           children_reply_1_embedding, children_reply_2_embedding
                    FROM conversations
                    WHERE development_stage = %s
                    AND (text ILIKE %s OR text ILIKE %s)
                    ORDER BY created_at DESC
                    LIMIT 1;
                """, (development_stage, f"%{user_input}%", f"%{user_input[:5]}%"))
                result = cur.fetchone()
                if result:
                    print(f"[ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢] è¦‹ã¤ã‹ã£ãŸä¾‹: '{result['text']}'")
                    return result
                return None
        except Exception as e:
            print(f"[æ¤œç´¢ã‚¨ãƒ©ãƒ¼] {e}")
            return None
    
    # OpenAIåˆ©ç”¨å¯èƒ½æ™‚ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    print(f"[ãƒ™ã‚¯ãƒˆãƒ«åŒ–] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}")
    try:
        resp = await openai_client.embeddings.create(
            input=[user_input],
            model="text-embedding-ada-002"
        )
        query_vector = resp.data[0].embedding
        print(f"[ãƒ™ã‚¯ãƒˆãƒ«åŒ–] å®Œäº†: (æ¬¡å…ƒ: {len(query_vector)})")
        
        with pg_conn.cursor() as cur:
            cur.execute("""
                SELECT text, fish_text, children_reply_1, children_reply_2,
                       children_reply_1_embedding, children_reply_2_embedding,
                       user_embedding <-> %s::vector as distance
                FROM conversations
                WHERE development_stage = %s
                AND user_embedding IS NOT NULL
                ORDER BY distance
                LIMIT 1;
            """, (query_vector, development_stage))
            result = cur.fetchone()
            if result:
                print(f"[é¡ä¼¼æ¤œç´¢] è¦‹ã¤ã‹ã£ãŸä¾‹: '{result['text']}'")
                print(f"[é¡ä¼¼æ¤œç´¢] é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {result['distance']:.4f}")
                return result
            else:
                print(f"[é¡ä¼¼æ¤œç´¢] {development_stage}ã«è©²å½“ã™ã‚‹ä¾‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
    except Exception as e:
        print(f"[ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼] {e}")
        return None

def get_medaka_reply(user_input, health_status="ä¸æ˜", conversation_hist=None, similar_example=None, profile_info=None):
    start = time.time()
    
    # å¥åº·çŠ¶æ…‹ã®å¤‰æ›
    if health_status == "Active":
        medaka_state = "å…ƒæ°—"
    elif health_status == "Normal":
        medaka_state = "ä¼‘æ†©ä¸­"
    elif health_status == "Lethargic":
        medaka_state = "å…ƒæ°—ãªã„"
    else:
        medaka_state = "ä¼‘æ†©ä¸­"
    
    print("ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹:", medaka_state)
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®å‡¦ç†
    profile_context = ""
    if profile_info:
        profile_name = profile_info.get('name', 'Unknown')
        age_text = f"{profile_info['age']}æ­³" if profile_info.get('age') else "å¹´é½¢ä¸æ˜"
        stage_text = profile_info.get('development_stage', 'ä¸æ˜')
        profile_context = f"è©±ã—ç›¸æ‰‹: {profile_name}ã•ã‚“ ({age_text}, {stage_text})\n"
    
    # ä¼šè©±å±¥æ­´ã®å‡¦ç†
    history_context = ""
    if conversation_hist and len(conversation_hist) > 0:
        recent_history = conversation_hist[-3:]
        history_context = "æœ€è¿‘ã®ä¼šè©±å±¥æ­´:\n"
        for i, h in enumerate(recent_history, 1):
            history_context += f"{i}. å…ç«¥ã€Œ{h['child']}ã€â†’ ãƒ¡ãƒ€ã‚«ã€Œ{h['medaka']}ã€\n"
        history_context += "\n"
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    if similar_example:
        prompt = f"""
        ã‚ãªãŸã¯æ°´æ§½ã«ä½ã‚€ã‹ã‚ã„ã„ãƒ¡ãƒ€ã‚«ã€Œã‚­ãƒ³ã¡ã‚ƒã‚“ã€ã§ã™ã€‚
        {profile_context}
        ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€å…¨ãåŒã˜è¨€è‘‰ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚
        ã€ä¼šè©±ã€‘
        å…ç«¥:ã€Œ{similar_example['text']}ã€
        ãƒ¡ãƒ€ã‚«:ã€Œ{similar_example['fish_text']}ã€

        {history_context}ã€ç¾åœ¨ã®ä¼šè©±ã€‘
        å…ç«¥:ã€Œ{user_input}ã€
        ãƒ¡ãƒ€ã‚«:
        """
    else:
        prompt = f"""
        ã‚ãªãŸã¯æ°´æ§½ã«ä½ã‚€ã‹ã‚ã„ã„ãƒ¡ãƒ€ã‚«ã€Œã‚­ãƒ³ã¡ã‚ƒã‚“ã€ã§ã™ã€‚
        {profile_context}{history_context}
        å…ç«¥:ã€Œ{user_input}ã€

        30æ–‡å­—ä»¥å†…ã§ã€å„ªã—ãå°å­¦ç”Ÿã‚‰ã—ã„å£èª¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚
        ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹: {medaka_state}

        ã‚­ãƒ³ã¡ã‚ƒã‚“:"""

    print(f"[å¿œç­”ç”Ÿæˆ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆå®Œäº†")
    
    # å¿œç­”ç”Ÿæˆï¼ˆGeminiåˆ©ç”¨å¯èƒ½æ™‚ï¼‰
    if model_gemini:
        try:
            generation_config = {
                'temperature': 0.5,
                'top_p': 0.1,
                'top_k': 1
            }
            response = model_gemini.generate_content(prompt, generation_config=generation_config)
            reply = response.text.strip()
            end = time.time()
            print(f"[Geminiå¿œç­”ç”Ÿæˆ] æ‰€è¦æ™‚é–“: {end - start:.2f}ç§’")
            print(f"[å¿œç­”ç”Ÿæˆ] ç”Ÿæˆã•ã‚ŒãŸå¿œç­”: '{reply}'")
            return reply
        except Exception as e:
            print(f"[Geminiå¿œç­”ã‚¨ãƒ©ãƒ¼] {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ«
    
    # Geminiæœªè¨­å®šæ™‚ã®ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å¿œç­”
    if any(word in user_input for word in ['ã“ã‚“ã«ã¡ã¯', 'ãŠã¯ã‚ˆã†', 'ã“ã‚“ã°ã‚“ã¯']):
        return f"ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯{medaka_state}ã ã‚ˆã€œ"
    elif any(word in user_input for word in ['å…ƒæ°—', 'ã’ã‚“ã']):
        return f"ã†ã‚“ã€{medaka_state}ã«ã—ã¦ã‚‹ã‚ˆï¼"
    elif any(word in user_input for word in ['æ³³ã', 'ãŠã‚ˆã', 'æ³³ã„']):
        return "ä¸€ç·’ã«æ³³ã”ã†ã­ã€œã‚¹ã‚¤ã‚¹ã‚¤ã€œ"
    elif any(word in user_input for word in ['ã‚ã‚ŠãŒã¨ã†', 'ã‚ã‚ŠãŒã¨']):
        return "ã©ã†ã„ãŸã—ã¾ã—ã¦ã€œã¾ãŸéŠã¼ã†ã­ï¼"
    elif any(word in user_input for word in ['å¥½ã', 'ã™ã']):
        return "ã‚ãŸã—ã‚‚å¤§å¥½ãã ã‚ˆã€œâ™ª"
    else:
        return f"ã†ã‚“ã†ã‚“ã€ãã†ãªã‚“ã ã€œ{medaka_state}ã«èã„ã¦ã‚‹ã‚ˆï¼"

def log_conversation(user_input, kinchan_reply):
    try:
        file_exists = os.path.isfile(CONVERSATION_LOG_FILE)
        with open(CONVERSATION_LOG_FILE, "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            if not file_exists:
                writer.writerow(["timestamp", "user_input", "kinchan_reply"])
            writer.writerow([datetime.now().isoformat(), user_input, kinchan_reply])
        print(f"[CSVãƒ­ã‚°] ä¿å­˜æˆåŠŸ: {user_input[:10]}...")
    except Exception as e:
        print(f"[CSVãƒ­ã‚°] ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# ä¼šè©±åˆ†é¡ï¼ˆæ¡ä»¶ä»˜ãå®Ÿè¡Œï¼‰
async def classify_child_response(child_response: str, similar_conversation: dict, openai_client, threshold: float = 0.5):
    if not openai_client:
        print("[ç™ºé”æ®µéšåˆ¤å®š] OpenAIæœªè¨­å®šã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        return "ç¾çŠ¶ç¶­æŒ", 0.0, 0.0
    
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] å…ç«¥ã®å¿œç­”: '{child_response}'")
    try:
        resp = await openai_client.embeddings.create(
            input=[child_response],
            model="text-embedding-ada-002"
        )
        response_vector = np.array(resp.data[0].embedding)
        
        # ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›é–¢æ•°
        def convert_to_vector(embedding_data):
            if isinstance(embedding_data, str):
                import json
                return np.array(json.loads(embedding_data), dtype=float)
            return np.array(embedding_data, dtype=float)
        
        maintain_vector = convert_to_vector(similar_conversation['children_reply_1_embedding'])
        upgrade_vector = convert_to_vector(similar_conversation['children_reply_2_embedding'])
        
        def cosine_similarity(v1, v2):
            if len(v1) != len(v2):
                return 0.0
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            if norm1 == 0 or norm2 == 0:
                return 0.0
            return np.dot(v1, v2) / (norm1 * norm2)
        
        maintain_similarity = cosine_similarity(response_vector, maintain_vector)
        upgrade_similarity = cosine_similarity(response_vector, upgrade_vector)
        
        result = "æ˜‡æ ¼" if upgrade_similarity > maintain_similarity and upgrade_similarity > threshold else "ç¾çŠ¶ç¶­æŒ"
        confidence = abs(upgrade_similarity - maintain_similarity)
        
        print(f"[ç™ºé”æ®µéšåˆ¤å®š] çµæœ: {result} (ä¿¡é ¼åº¦: {confidence:.4f})")
        return result, maintain_similarity, upgrade_similarity
        
    except Exception as e:
        print(f"[ç™ºé”æ®µéšåˆ¤å®šã‚¨ãƒ©ãƒ¼] {e}")
        return "ç¾çŠ¶ç¶­æŒ", 0.0, 0.0

@app.post("/talk_with_fish_text")
async def talk_with_fish_text(request: Request):
    start_total = time.time()
    data = await request.json()
    user_input = data.get("user_input", "")
    
    if not user_input.strip():
        raise HTTPException(400, "user_input is required")
    
    try:
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
        with pg_conn.cursor() as cur:
            cur.execute("SELECT * FROM profiles WHERE id = %s;", (CURRENT_PROFILE_ID,))
            profile = cur.fetchone()
            if not profile:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
                profile = {"id": 1, "name": "ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼", "development_stage": "stage_1"}
        
        current_stage = profile["development_stage"]
        
        # ä¼šè©±å±¥æ­´ã®å–å¾—/åˆæœŸåŒ–
        if CURRENT_PROFILE_ID not in conversation_history:
            conversation_history[CURRENT_PROFILE_ID] = []
        current_history = conversation_history[CURRENT_PROFILE_ID]
        
        # å‰å›ã®é¡ä¼¼ä¾‹ãŒã‚ã‚‹ã‹
        similar_example = last_similar_example[CURRENT_PROFILE_ID]
        assessment_result = None
        
        if similar_example is None:
            # 1å›ç›®ã®ä¼šè©±ï¼šé¡ä¼¼ä¾‹ã‚’æ¤œç´¢
            print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 1å›ç›®ã®ä¼šè©± - é¡ä¼¼ä¾‹ã‚’æ¤œç´¢")
            similar_example = await find_similar_conversation(user_input, current_stage)
            
            # é¡ä¼¼ä¾‹ã‚’ä¿å­˜ï¼ˆæ¬¡å›ã®åˆ¤å®šç”¨ï¼‰
            if similar_example and 'children_reply_1_embedding' in similar_example:
                last_similar_example[CURRENT_PROFILE_ID] = similar_example
                print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] é¡ä¼¼ä¾‹ã‚’ä¿å­˜ - æ¬¡å›ç™ºé”æ®µéšåˆ¤å®šäºˆå®š")
        else:
            # 2å›ç›®ã®ä¼šè©±ã®å ´åˆã€ç™ºé”æ®µéšåˆ¤å®šã‚’å®Ÿè¡Œ
            print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 2å›ç›®ã®ä¼šè©± - ç™ºé”æ®µéšåˆ¤å®šã‚’å®Ÿè¡Œ")
            
            # å…ç«¥ã®å¿œç­”åˆ†é¡
            assessment = await classify_child_response(user_input, similar_example, openai_client)
            
            assessment_result = {
                'result': assessment[0],
                'maintain_score': float(assessment[1]),
                'upgrade_score': float(assessment[2]),
                'assessed_at': datetime.now(),
            }
            
            # é¡ä¼¼ä¾‹ã‚’ã‚¯ãƒªã‚¢
            last_similar_example[CURRENT_PROFILE_ID] = None
            similar_example = None
        
        # å¿œç­”ç”Ÿæˆ
        reply_text = get_medaka_reply(user_input, latest_health, current_history, similar_example, profile)
        
        # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        conversation_entry = {
            "child": user_input,
            "medaka": reply_text,
            "timestamp": datetime.now(),
            "similar_example_used": similar_example['text'] if similar_example else None,
            "has_assessment": assessment_result is not None,
            "assessment_result": assessment_result
        }
        conversation_history[CURRENT_PROFILE_ID].append(conversation_entry)
        
        if len(conversation_history[CURRENT_PROFILE_ID]) > 20:
            conversation_history[CURRENT_PROFILE_ID] = conversation_history[CURRENT_PROFILE_ID][-20:]
        
        # CSVãƒ­ã‚°ä¿å­˜
        log_conversation(user_input, reply_text)
        
        # éŸ³å£°åˆæˆï¼ˆOpenAIåˆ©ç”¨å¯èƒ½æ™‚ï¼‰
        if openai_client:
            try:
                async with openai_client.audio.speech.with_streaming_response.create(
                    model="tts-1",
                    voice="nova",
                    input=reply_text,
                    response_format="mp3",
                ) as response:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tts_file:
                        async for chunk in response.iter_bytes():
                            tts_file.write(chunk)
                        tts_path = tts_file.name
                
                end_total = time.time()
                print(f"[ç·å‡¦ç†æ™‚é–“] {end_total - start_total:.2f}ç§’")
                return FileResponse(tts_path, media_type="audio/mpeg", filename="reply.mp3")
            except Exception as e:
                print(f"[éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼] {e}")
                return {"reply": reply_text, "audio": False}
        else:
            return {"reply": reply_text, "audio": False}
        
    except Exception as e:
        print(f"[ä¼šè©±ã‚¨ãƒ©ãƒ¼] {e}")
        raise HTTPException(500, f"Internal server error: {str(e)}")

@app.post("/predict")
async def predict(file: UploadFile):
    global latest_health
    
    try:
        file_content = await file.read()
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            files = {"file": (file.filename, io.BytesIO(file_content), file.content_type)}
            response = await client.post(f"{INFERENCE_SERVER_URL}/predict", files=files)
            
            if response.status_code != 200:
                raise HTTPException(status_code=response.status_code, 
                                  detail=f"Inference server error: {response.text}")
            
            health_status = response.headers.get("X-Health-Status", "Unknown")
            latest_health = health_status
            
            return StreamingResponse(
                io.BytesIO(response.content),
                media_type="image/jpeg"
            )
            
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="Inference server timeout")
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="Cannot connect to inference server")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

@app.get("/")
async def read_index():
    return {"message": "Medaka Fish App is running!", "status": "healthy"}

@app.get("/health")
async def health_check():
    try:
        with pg_conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) as count FROM conversations")
            count = cur.fetchone()['count']
            return {
                "status": "healthy", 
                "database": "connected",
                "conversations": count,
                "openai": "configured" if openai_client else "not configured",
                "gemini": "configured" if model_gemini else "not configured"
            }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

# ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/conversation_history")
async def get_conversation_history():
    if CURRENT_PROFILE_ID in conversation_history:
        return {
            "profile_id": CURRENT_PROFILE_ID,
            "history": list(conversation_history[CURRENT_PROFILE_ID])
        }
    else:
        return {"profile_id": CURRENT_PROFILE_ID, "history": []}

@app.delete("/conversation_history")
async def clear_conversation_history():
    if CURRENT_PROFILE_ID in conversation_history:
        del conversation_history[CURRENT_PROFILE_ID]
    return {"message": f"History cleared for profile {CURRENT_PROFILE_ID}"}

@app.post("/test_vector_search")
async def test_vector_search(request: Request):
    data = await request.json()
    user_input = data.get("user_input", "")
    stage = data.get("stage", "stage_1")
    
    result = await find_similar_conversation(user_input, stage)
    return {"query": user_input, "stage": stage, "result": result}

if __name__ == "__main__":
    print("ğŸš€ Medaka Fish App starting...")
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))