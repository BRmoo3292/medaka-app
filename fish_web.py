from fastapi import FastAPI, UploadFile, HTTPException, Request
from fastapi.responses import FileResponse,Response,StreamingResponse
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
import time
import numpy as np
import asyncio
from collections import defaultdict, deque
import tempfile
import os
import google.generativeai as genai
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®URLã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
DB_URL = os.getenv("DB_URL")
pg_conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
pg_conn.autocommit = True
print(f"[èµ·å‹•æ™‚] DBæ¥ç¶šæˆåŠŸ: {DB_URL}")

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"]  
)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)
model_gemini = genai.GenerativeModel(model_name="gemini-2.0-flash")

print(f"[èµ·å‹•æ™‚] DB_URLè¨­å®š: {'ã‚ã‚Š' if DB_URL else 'ãªã—'}")
print(f"[èµ·å‹•æ™‚] OpenAI API: {'è¨­å®šæ¸ˆã¿' if OPENAI_API_KEY else 'æœªè¨­å®š'}")
print(f"[èµ·å‹•æ™‚] Gemini API: {'è¨­å®šæ¸ˆã¿' if GEMINI_API_KEY else 'æœªè¨­å®š'}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
active_session = {}
conversation_history = defaultdict(lambda: deque(maxlen=10))
latest_health = "Normal"
CURRENT_PROFILE_ID = 1

# Session Poolerå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šé–¢æ•°
def connect_to_database(db_url, max_retries=3):
    if "pooler.supabase.com" in db_url:
        print("[DBæ¥ç¶š] Supabase Pooleræ¥ç¶šã‚’ä½¿ç”¨")
        if ":5432" in db_url:
            print("[DBæ¥ç¶š] Session Pooler (ãƒãƒ¼ãƒˆ5432)")
        elif ":6543" in db_url:
            print("[DBæ¥ç¶š] Transaction Pooler (ãƒãƒ¼ãƒˆ6543)")
        
        if "sslmode=" not in db_url:
            if "?" in db_url:
                db_url += "&sslmode=require"
            else:
                db_url += "?sslmode=require"
    
    print(f"[DBæ¥ç¶š] æ¥ç¶šå…ˆ: {db_url.split('@')[0]}@...")
    
    for attempt in range(max_retries):
        try:
            print(f"[DBæ¥ç¶š] è©¦è¡Œ {attempt + 1}/{max_retries}")
            
            conn = psycopg2.connect(
                db_url,
                cursor_factory=RealDictCursor,
                keepalives=1,
                keepalives_idle=30,
                keepalives_interval=10,
                keepalives_count=5,
                connect_timeout=10
            )
            
            conn.autocommit = True
            
            with conn.cursor() as cur:
                cur.execute("SELECT 1")
                result = cur.fetchone()
                if result:
                    print(f"âœ… DBæ¥ç¶šæˆåŠŸ! (è©¦è¡Œ {attempt + 1})")
                    
                    cur.execute("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        LIMIT 5;
                    """)
                    tables = cur.fetchall()
                    print(f"[DBæƒ…å ±] æ¤œå‡ºã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«: {[t['table_name'] for t in tables]}")
                    
                    return conn
                    
        except psycopg2.OperationalError as e:
            error_msg = str(e)
            print(f"âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {error_msg}")
            
            if "password authentication failed" in error_msg:
                print("[ã‚¨ãƒ©ãƒ¼] ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
                break
            elif "Network is unreachable" in error_msg:
                print("[ã‚¨ãƒ©ãƒ¼] IPv6æ¥ç¶šã®å•é¡Œã§ã™ã€‚Session Poolerã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
                break
            
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"[DBæ¥ç¶š] {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                time.sleep(wait_time)
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            break
    
    return None

@app.get("/best.onnx")
async def serve_onnx_model():
    """ãƒ–ãƒ©ã‚¦ã‚¶æ¤œå‡ºç”¨ã®ONNXãƒ¢ãƒ‡ãƒ«ã‚’é…ä¿¡"""
    model_path = "best.onnx"
    if not os.path.exists(model_path):
        raise HTTPException(404, f"Model file not found: {model_path}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(model_path, "rb") as f:
        content = f.read()
    
    # Responseã§ç›´æ¥è¿”ã™ï¼ˆCORSãƒ˜ãƒƒãƒ€ãƒ¼å®Œå…¨åˆ¶å¾¡ï¼‰
    return Response(
        content=content,
        media_type="application/octet-stream",
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, OPTIONS",
            "Access-Control-Allow-Headers": "*",
            "Access-Control-Expose-Headers": "*",
            "Cache-Control": "public, max-age=31536000",
            "Content-Type": "application/octet-stream",
            "Content-Length": str(len(content))
        }
    )

@app.options("/best.onnx")
async def options_onnx_model():
    return Response(
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, OPTIONS",
            "Access-Control-Allow-Headers": "*"
        }
    )


@app.post("/transcribe_audio")
async def transcribe_audio(file: UploadFile):
    """Whisper APIã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
    start = time.time()
    
    try:
        audio_content = await file.read()
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_audio:
            temp_audio.write(audio_content)
            temp_audio_path = temp_audio.name
        
        with open(temp_audio_path, "rb") as audio_file:
            transcript = await openai_client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ja",
                response_format="verbose_json"
            )
        
        os.unlink(temp_audio_path)
        
        end = time.time()
        print(f"[Whisper] æ–‡å­—èµ·ã“ã—å®Œäº†: '{transcript.text}' ({end - start:.2f}ç§’)")
        
        return {
            "text": transcript.text,
            "duration": transcript.duration,
            "language": transcript.language
        }
        
    except Exception as e:
        print(f"[Whisper] ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"Transcription error: {str(e)}")

@app.post("/talk_with_fish_audio")
async def talk_with_fish_audio(file: UploadFile):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘å–ã‚Šã€Whisperã§æ–‡å­—èµ·ã“ã—å¾Œã€ãƒ¡ãƒ€ã‚«ã®å¿œç­”ã‚’è¿”ã™"""
    start_total = time.time()
    
    try:
        # 1. Whisperã§æ–‡å­—èµ·ã“ã—
        transcription_result = await transcribe_audio(file)
        user_input = transcription_result["text"]
        
        if not user_input.strip():
            raise HTTPException(400, "No speech detected")
        
        print(f"[éŸ³å£°èªè­˜] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: '{user_input}'")
        
        # 2. ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
        with pg_conn.cursor() as cur:
            cur.execute("SELECT * FROM profiles WHERE id = %s;", (CURRENT_PROFILE_ID,))
            profile = cur.fetchone()
            if not profile:
                raise HTTPException(404, "Profile not found")
            current_stage = profile["development_stage"]
        
        # ä¼šè©±å±¥æ­´å–å¾—
        if CURRENT_PROFILE_ID not in conversation_history:
            conversation_history[CURRENT_PROFILE_ID] = []
        current_history = conversation_history[CURRENT_PROFILE_ID]
        
        session = active_session.get(CURRENT_PROFILE_ID)
        assessment_result = None
        similar_example = None
        reply_text = None
        
        if session is None:
            # 1å›ç›®ã®ä¼šè©±
            print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 1å›ç›®ã®ä¼šè©± - é¡ä¼¼ä¾‹ã‚’æ¤œç´¢")
            similar_example = await find_similar_conversation(user_input, current_stage)
            reply_text = get_medaka_reply(user_input, latest_health, current_history, similar_example, profile)
            
            if (similar_example and 
                'child_reply_1_embedding' in similar_example and 
                similar_example['distance'] < 0.5):
                session = ConversationSession(
                    profile_id=CURRENT_PROFILE_ID,
                    first_input=user_input,
                    medaka_response=reply_text,
                    similar_example=similar_example,
                    current_stage=current_stage
                )
                active_session[CURRENT_PROFILE_ID] = session
        else:
            # 2å›ç›®ã®ä¼šè©± - ç™ºé”æ®µéšåˆ¤å®š
            print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 2å›ç›®ã®ä¼šè©± - ç™ºé”æ®µéšåˆ¤å®šã‚’å®Ÿè¡Œ")
            assessment = await classify_child_response(
                user_input,
                session.similar_example,
                openai_client,
            )
            assessment_result = {
                'result': assessment[0],
                'maintain_score': round(float(assessment[1]), 3),
                'upgrade_score': round(float(assessment[2]), 3),
                'confidence_score': round(float(abs(assessment[2] - assessment[1])), 5),
                'assessed_at': datetime.now(),
            }
            reply_text = get_medaka_reply(user_input, latest_health, current_history, None, profile)
            session_id = session.complete_session(user_input, assessment)
            del active_session[CURRENT_PROFILE_ID]
        
        if not reply_text:
            raise HTTPException(500, "Reply text generation failed")
        
        # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        conversation_entry = {
            "child": user_input,
            "medaka": reply_text,
            "timestamp": datetime.now(),
            "similar_example_used": similar_example['text'] if similar_example else None,
            "similarity_score": similar_example['distance'] if similar_example else None,
            "has_assessment": assessment_result is not None,
            "assessment_result": assessment_result,
            "session_status": "started" if session and CURRENT_PROFILE_ID in active_session else "completed"
        }
        conversation_history[CURRENT_PROFILE_ID].append(conversation_entry)
        
        # TTSç”Ÿæˆï¼ˆå…ƒã®æ–¹æ³•ã«æˆ»ã™ï¼‰
        t_tts_start = time.time()
        
        async with openai_client.audio.speech.with_streaming_response.create(
            model="gpt-4o-mini-tts",
            voice="coral",
            instructions="""
            Voice Affect:ã®ã‚“ã³ã‚Šã—ã¦ã„ã¦ã€ã‹ã‚ã„ã‚‰ã—ã„ç„¡é‚ªæ°—ã•  
            Tone:ã»ã‚“ã‚ã‹ã€å°‘ã—ãŠã£ã¨ã‚Šã€è¦ªã—ã¿ã‚„ã™ã„  
            Pacing:å…¨ä½“çš„ã«ã‚†ã£ãã‚Šã‚ã€è¨€è‘‰ã¨è¨€è‘‰ã®é–“ã«ä½™è£•ã‚’æŒãŸã›ã‚‹  
            """,
            speed=1.0,
            input=reply_text,
            response_format="mp3",
        ) as response:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tts_file:
                async for chunk in response.iter_bytes():
                    tts_file.write(chunk)
                tts_path = tts_file.name
        
        t_tts_end = time.time()
        print(f"[TTSç”Ÿæˆ] {t_tts_end - t_tts_start:.2f}ç§’")
        
        end_total = time.time()
        print(f"[ç·å‡¦ç†æ™‚é–“] {end_total - start_total:.2f}ç§’")
        
        return FileResponse(tts_path, media_type="audio/mpeg", filename="reply.mp3")
        
    except Exception as e:
        print(f"[éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼] {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def generate_tts(text: str) -> str:
    """TTSç”Ÿæˆï¼ˆéåŒæœŸé–¢æ•°ï¼‰"""
    async with openai_client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",
        voice="coral",
        instructions="""
        Voice Affect:ã®ã‚“ã³ã‚Šã—ã¦ã„ã¦ã€ã‹ã‚ã„ã‚‰ã—ã„ç„¡é‚ªæ°—ã•  
        Tone:ã»ã‚“ã‚ã‹ã€å°‘ã—ãŠã£ã¨ã‚Šã€è¦ªã—ã¿ã‚„ã™ã„  
        Pacing:å…¨ä½“çš„ã«ã‚†ã£ãã‚Šã‚ã€è¨€è‘‰ã¨è¨€è‘‰ã®é–“ã«ä½™è£•ã‚’æŒãŸã›ã‚‹  
        """,
        speed=1.0,
        input=text,
        response_format="mp3",
    ) as response:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tts_file:
            async for chunk in response.iter_bytes():
                tts_file.write(chunk)
            return tts_file.name
        

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
try:
    pg_conn = connect_to_database(DB_URL)
    if not pg_conn:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ç¢ºç«‹ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        exit(1)
except Exception as e:
    print(f"âŒ DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
    exit(1)

# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é–¢æ•°
async def find_similar_conversation(user_input: str, development_stage: str):
    print(f"[ãƒ™ã‚¯ãƒˆãƒ«åŒ–] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}")
    resp = await openai_client.embeddings.create(
        input=[user_input],
        model="text-embedding-ada-002"
    )
    query_vector = resp.data[0].embedding
    print(f"[ãƒ™ã‚¯ãƒˆãƒ«åŒ–]å®Œäº†:(æ¬¡å…ƒ: {len(query_vector)})")
    
    with pg_conn.cursor() as cur:
        cur.execute("""
            SELECT text, fish_text, children_reply_1, children_reply_2,
                   child_reply_1_embedding, child_reply_2_embedding,
                   user_embedding <-> %s::vector as distance
            FROM conversations
            WHERE development_stage = %s
            ORDER BY distance
            LIMIT 1;
        """, (query_vector, development_stage))
        result = cur.fetchone()
        if result:
            print(f"[é¡ä¼¼æ¤œç´¢] è¦‹ã¤ã‹ã£ãŸä¾‹: '{result['text']}'")
            print(f"[é¡ä¼¼æ¤œç´¢] é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {result['distance']:.4f}")
            return result
        else:
            print(f"[é¡ä¼¼æ¤œç´¢] {development_stage}ã«è©²å½“ã™ã‚‹ä¾‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None

def get_medaka_reply(user_input, healt_status="ä¸æ˜", conversation_hist=None, similar_example=None, profile_info=None):
    start = time.time()
    if healt_status == "Active":
        medaka_state = "å…ƒæ°—"
    elif healt_status == "Normal":
        medaka_state = "ä¼‘æ†©ä¸­"
    elif healt_status == "Lethargic":
        medaka_state = "å…ƒæ°—ãªã„"
    else:
        medaka_state = "ä¼‘æ†©ä¸­"
    print("ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹:", medaka_state)
    
    if profile_info:
        profile_name = profile_info.get('name', 'Unknown')
        age_text = f"{profile_info['age']}æ­³" if profile_info.get('age') else "å¹´é½¢ä¸æ˜"
        stage_text = profile_info.get('development_stage', 'ä¸æ˜')
        profile_context = f"è©±ã—ç›¸æ‰‹: {profile_name}ã•ã‚“ ({age_text}, {stage_text})\n"
        history_context = ""
        if conversation_hist and len(conversation_hist) > 0:
            recent_history = conversation_hist[-3:]
            history_context = "æœ€è¿‘ã®ä¼šè©±å±¥æ­´:\n"
            for i, h in enumerate(recent_history, 1):
                history_context += f"{i}. å…ç«¥ã€Œ{h['child']}ã€â†’ ãƒ¡ãƒ€ã‚«ã€Œ{h['medaka']}ã€\n"
        history_context += "\n"
       
    if similar_example:
        prompt = f"""
                ã‚ãªãŸã¯æ°´æ§½ã«ä½ã‚€ã‹ã‚ã„ã„ãƒ¡ãƒ€ã‚«ã€Œã‚­ãƒ³ã¡ã‚ƒã‚“ã€ã§ã™ã€‚
                ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹: {medaka_state}
                {profile_context}
                ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€å…¨ãåŒã˜è¨€è‘‰ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚
                ã€ä¼šè©±ã€‘
                å…ç«¥:ã€Œ{similar_example['text']}ã€
                ãƒ¡ãƒ€ã‚«:ã€Œ{similar_example['fish_text']}ã€

                {history_context}ã€ç¾åœ¨ã®ä¼šè©±ã€‘
                å…ç«¥:ã€Œ{user_input}ã€
                ãƒ¡ãƒ€ã‚«:
                """
    else:
        prompt = f"""
        ã‚ãªãŸã¯æ°´æ§½ã«ä½ã‚€ã‹ã‚ã„ã„ãƒ¡ãƒ€ã‚«ã€Œã‚­ãƒ³ã¡ã‚ƒã‚“ã€ã§ã™ã€‚
        {profile_context}{history_context}
        å…ç«¥:ã€Œ{user_input}ã€

        30æ–‡å­—ä»¥å†…ã§ã€å„ªã—ãå°å­¦ç”Ÿã‚‰ã—ã„å£èª¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚
        ãƒ¡ãƒ€ã‚«ã®çŠ¶æ…‹: {medaka_state}

        ã‚­ãƒ³ã¡ã‚ƒã‚“:"""

    print(f"[å¿œç­”ç”Ÿæˆ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆå®Œäº†", prompt)
    generation_config = genai.types.GenerationConfig(
        temperature=0.5,
        top_p=0.1,
        top_k=1
    )
    response = model_gemini.generate_content(
        prompt,
        generation_config=generation_config
    )
    end = time.time()

    reply = response.text.strip()

    print(f"[Geminiå¿œç­”ç”Ÿæˆ] æ‰€è¦æ™‚é–“: {end - start:.2f}ç§’")
    print(f"[å¿œç­”ç”Ÿæˆ] ç”Ÿæˆã•ã‚ŒãŸå¿œç­”: '{reply}'")
    return reply

class ConversationSession:
    def __init__(self, profile_id: int, first_input: str, medaka_response: str, similar_example: dict, current_stage: str):
        self.profile_id = profile_id
        self.first_child_input = first_input
        self.medaka_response = medaka_response
        self.similar_example = similar_example
        self.current_stage = current_stage
        self.stared_at = datetime.now()

    def complete_session(self, second_input: str, assessment_result: tuple):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Œäº†ã—ã€DBã«ä¿å­˜"""
        self.second_child_input = second_input
        self.assessment_result = assessment_result[0]
        self.maintain_score = round(float(assessment_result[1]), 3)
        self.upgrade_score = round(float(assessment_result[2]), 3)
        self.confidence_score = round(float(abs(self.upgrade_score - self.maintain_score)), 5)
        
        return self._save_to_database()
    
    def _save_to_database(self) -> int:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            with pg_conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO conversation_sessions (
                        profile_id, first_child_input, medaka_response, second_child_input,
                        assessment_result, maintain_similarity_score, upgrade_similarity_score, 
                        confidence_score, current_stage
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s
                    ) RETURNING id;
                """, (
                    self.profile_id,
                    self.first_child_input,
                    self.medaka_response,
                    self.second_child_input,
                    self.assessment_result,
                    self.maintain_score,      
                    self.upgrade_score,       
                    self.confidence_score,   
                    self.current_stage
                ))
                
                session_id = cur.fetchone()['id']
                print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] ä¿å­˜å®Œäº† ID: {session_id}")
                print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] åˆ¤å®šçµæœ: {self.assessment_result} (ä¿¡é ¼åº¦: {self.confidence_score:.3f})")
                
                return session_id
                
        except Exception as e:
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³DB] ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None

STAGE_PROGRESSION = {
    "stage_1": "stage_2",
    "stage_2": "stage_3",
    "stage_3": "stage_3"
}

def upgrade_development_stage(profile_id: int, current_stage: str) -> str:
    """ç™ºé”æ®µéšã‚’1ã¤ä¸Šã’ã‚‹"""
    next_stage = STAGE_PROGRESSION.get(current_stage, current_stage)
    
    if next_stage == current_stage:
        print(f"[ç™ºé”æ®µéš] ã™ã§ã«æœ€é«˜æ®µéš: {current_stage}")
        return current_stage
    
    try:
        with pg_conn.cursor() as cur:
            cur.execute("""
                UPDATE profiles 
                SET development_stage = %s,
                    updated_at = NOW()
                WHERE id = %s
                RETURNING development_stage;
            """, (next_stage, profile_id))
            
            result = cur.fetchone()
            
            if result:
                print(f"[ç™ºé”æ®µéš] æ˜‡æ ¼æˆåŠŸ: {current_stage} â†’ {next_stage} (Profile ID: {profile_id})")
                return next_stage
            else:
                print(f"[ç™ºé”æ®µéš] ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: Profile ID {profile_id}")
                return current_stage
                
    except Exception as e:
        print(f"[ç™ºé”æ®µéš] æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return current_stage

# ä¼šè©±åˆ†é¡
async def classify_child_response(
        child_response: str,
        similar_conversation: dict,
        openai_client,
        threshold: float = 0.5
) -> tuple[str, float, float]:
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] å…ç«¥ã®å¿œç­”: '{child_response}'")
    
    resp = await openai_client.embeddings.create(
        input=[child_response],
        model="text-embedding-ada-002"
    )
    response_vector = np.array(resp.data[0].embedding)
    
    def convert_to_vector(embedding_data):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        if isinstance(embedding_data, str):
            import json
            return np.array(json.loads(embedding_data), dtype=float)
            
    maintain_vector = convert_to_vector(similar_conversation['child_reply_1_embedding'])
    upgrade_vector = convert_to_vector(similar_conversation['child_reply_2_embedding'])
    
    def cosine_similarity(v1, v2):
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        if len(v1) != len(v2):
            raise ValueError(f"ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒãŒä¸€è‡´ã—ã¾ã›ã‚“: {len(v1)} vs {len(v2)}")
        
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return np.dot(v1, v2) / (norm1 * norm2)
    
    maintain_similarity = cosine_similarity(response_vector, maintain_vector)
    upgrade_similarity = cosine_similarity(response_vector, upgrade_vector)
    
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] ç¾çŠ¶ç¶­æŒã¨ã®é¡ä¼¼åº¦: {maintain_similarity:.4f}")
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] æ˜‡æ ¼ã¨ã®é¡ä¼¼åº¦: {upgrade_similarity:.4f}")
    
    if upgrade_similarity > maintain_similarity and upgrade_similarity > threshold:
        result = "æ˜‡æ ¼"
    else:
        result = "ç¾çŠ¶ç¶­æŒ"
    
    confidence = abs(upgrade_similarity - maintain_similarity)
    print(f"[ç™ºé”æ®µéšåˆ¤å®š] çµæœ: {result} (ä¿¡é ¼åº¦: {confidence:.4f})")
    
    return result, maintain_similarity, upgrade_similarity

@app.post("/talk_with_fish_text")
async def talk_with_fish_text(request: Request):
    start_total = time.time()
    data = await request.json()
    user_input = data.get("user_input", "")
    if not user_input.strip():
        raise HTTPException(400, "user_input is required")
    
    with pg_conn.cursor() as cur:
        cur.execute("SELECT * FROM profiles WHERE id = %s;", (CURRENT_PROFILE_ID,))
        profile = cur.fetchone()
        if not profile:
            raise HTTPException(404, "Profile not found")
        current_stage = profile["development_stage"]
    
    if CURRENT_PROFILE_ID not in conversation_history:
        conversation_history[CURRENT_PROFILE_ID] = []
    current_history = conversation_history[CURRENT_PROFILE_ID]
    session = active_session.get(CURRENT_PROFILE_ID)
    assessment_result = None  
    similar_example = None

    if session is None:
        print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 1å›ç›®ã®ä¼šè©± - é¡ä¼¼ä¾‹ã‚’æ¤œç´¢")
        similar_example = await find_similar_conversation(user_input, current_stage)
        reply_text = get_medaka_reply(user_input, latest_health, current_history, similar_example, profile)
        
        if (similar_example and 
            'child_reply_1_embedding' in similar_example and 
            similar_example['distance'] < 0.5):
            session = ConversationSession(
                    profile_id=CURRENT_PROFILE_ID,
                    first_input=user_input,
                    medaka_response=reply_text,
                    similar_example=similar_example,
                    current_stage=current_stage
            )
            active_session[CURRENT_PROFILE_ID] = session
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº† - æ¬¡å›åˆ¤å®šå®Ÿè¡Œäºˆå®šï¼ˆé¡ä¼¼åº¦: {similar_example['distance']:.4f}ï¼‰")
        else:
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] é¡ä¼¼åº¦ãŒä½ã„ - é€šå¸¸ã®ä¼šè©±ã¨ã—ã¦å‡¦ç†")
    else:
        print("[ä¼šè©±ãƒ•ãƒ­ãƒ¼] 2å›ç›®ã®ä¼šè©± - ç™ºé”æ®µéšåˆ¤å®šã‚’å®Ÿè¡Œ")
        
        assessment = await classify_child_response(
            user_input,
            session.similar_example,
            openai_client,
        )
        
        assessment_result = {
            'result': assessment[0],
            'maintain_score': round(float(assessment[1]), 3),
            'upgrade_score': round(float(assessment[2]), 3),
            'confidence_score': round(float(abs(assessment[2] - assessment[1])), 5),
            'assessed_at': datetime.now(),
        }
        
        if assessment[0] == "æ˜‡æ ¼":
            new_stage = upgrade_development_stage(CURRENT_PROFILE_ID, current_stage)
            profile["development_stage"] = new_stage
            
            if new_stage != current_stage:
                assessment_result['stage_upgraded'] = True
                assessment_result['previous_stage'] = current_stage
                assessment_result['new_stage'] = new_stage
                print(f"[ä¼šè©±ãƒ•ãƒ­ãƒ¼] ğŸ‰ ç™ºé”æ®µéšãŒæ˜‡æ ¼ã—ã¾ã—ãŸï¼ {current_stage} â†’ {new_stage}")
            else:
                assessment_result['stage_upgraded'] = False
                assessment_result['already_max'] = True
                print(f"[ä¼šè©±ãƒ•ãƒ­ãƒ¼] ã™ã§ã«æœ€é«˜æ®µéš {current_stage} ã«åˆ°é”ã—ã¦ã„ã¾ã™")
        else:
            assessment_result['stage_upgraded'] = False
            print(f"[ä¼šè©±ãƒ•ãƒ­ãƒ¼] ç¾çŠ¶ç¶­æŒ - {current_stage} ã®ã¾ã¾")
        
        reply_text = get_medaka_reply(user_input, latest_health, current_history, None, profile)
        session_id = session.complete_session(user_input, assessment)
        del active_session[CURRENT_PROFILE_ID]
        print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] åˆ¤å®šå®Œäº† - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {session_id}")

    conversation_entry = {
            "child": user_input,
            "medaka": reply_text,
            "timestamp": datetime.now(),
            "similar_example_used": similar_example['text'] if similar_example else None,
            "similarity_score": similar_example['distance'] if similar_example else None,
            "has_assessment": assessment_result is not None,
            "assessment_result": assessment_result,
            "session_status": "started" if session and CURRENT_PROFILE_ID in active_session else "completed"
    }
    conversation_history[CURRENT_PROFILE_ID].append(conversation_entry)
    if len(conversation_history[CURRENT_PROFILE_ID]) > 20:
        conversation_history[CURRENT_PROFILE_ID] = conversation_history[CURRENT_PROFILE_ID][-20:]

    print(f"[ä¼šè©±å±¥æ­´] ç¾åœ¨ã®å±¥æ­´ä»¶æ•°: {len(conversation_history[CURRENT_PROFILE_ID])}")
    async def audio_stream():
        async with openai_client.audio.speech.with_streaming_response.create(
            model="gpt-4o-mini-tts",
            voice="coral",
            instructions="""
            Voice Affect:ã®ã‚“ã³ã‚Šã—ã¦ã„ã¦ã€ã‹ã‚ã„ã‚‰ã—ã„ç„¡é‚ªæ°—ã•  
            Tone:ã»ã‚“ã‚ã‹ã€å°‘ã—ãŠã£ã¨ã‚Šã€è¦ªã—ã¿ã‚„ã™ã„  
            Pacing:å…¨ä½“çš„ã«ã‚†ã£ãã‚Šã‚ã€è¨€è‘‰ã¨è¨€è‘‰ã®é–“ã«ä½™è£•ã‚’æŒãŸã›ã‚‹  
            """,
            speed=1.0,
            input=reply_text,
            response_format="mp3",
        ) as response:
            async for chunk in response.iter_bytesn ():
                yield chunk 

        end_total = time.time()
        print(f"[ç·å‡¦ç†æ™‚é–“] {end_total - start_total:.2f}ç§’ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹ã¾ã§ï¼‰")
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
    return StreamingResponse(
            audio_stream(),
            media_type="audio/mpeg",
            headers={"Content-Disposition": "inline; filename=reply.mp3"}
        )

# âœ… ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰å…ƒæ°—åº¦ã‚’å—ä¿¡ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/update_health")
async def update_health(request: Request):
    """ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰é€ä¿¡ã•ã‚ŒãŸå…ƒæ°—åº¦ã‚’æ›´æ–°"""
    global latest_health
    
    data = await request.json()
    status = data.get("status", "Unknown")
    avg_speed = data.get("avg_speed", 0)
    score = data.get("score", 0)
    
    latest_health = status
    
    print(f"[å…ƒæ°—åº¦æ›´æ–°] {status} (é€Ÿåº¦: {avg_speed:.2f}px/s, ã‚¹ã‚³ã‚¢: {score})")
    
    return {
        "status": "success",
        "current_health": latest_health
    }

@app.get("/")
async def read_index():
    return FileResponse('index.html', media_type='text/html')

def get_proactive_medaka_message(conversation_count, profile):
    """ä¼šè©±å›æ•°ã«å¿œã˜ã¦ãƒ¡ãƒ€ã‚«ã‹ã‚‰ã®ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    messages = {
        0: ["ã¯ã˜ã‚ã¾ã—ã¦ï¼åƒ•ã€ãã‚“ã¡ã‚ƒã‚“ã ã‚ˆã€œå›ã®åå‰ã¯ãªã‚“ã¦è¨€ã†ã®ï¼Ÿ", "ã‚„ã£ã»ãƒ¼ï¼åƒ•ã¨ãŠè©±ã—ã—ãªã„ï¼Ÿ", "ä»Šæ—¥ã®å›ã¯ã€ã©ã‚“ãªä¸€æ—¥ã ã£ãŸï¼Ÿåƒ•ã¯ã­ã€æ°´è‰ã®ãƒ™ãƒƒãƒ‰ã§ãŠæ˜¼å¯ã—ã¦ãŸã‚“ã ã‚ˆ"],
        1: ["ã“ã‚“ã«ã¡ã¯ï¼ãã‚‡ã†ã¯ä½•ã‚’ã—ã¦éŠã‚“ã ã®ï¼Ÿ ã¼ãã¯ã­ã€æ°´ã®ä¸­ã§ã‚†ã‚‰ã‚†ã‚‰æºã‚Œã‚‹ã®ãŒå¥½ãã ã‚ˆã€‚", "ã²ã¾ã ã‚ˆã€œ!ä¸€ç·’ã«ãŠè©±ã—ã—ã‚ˆï¼", "ä»Šä½•ã—ã¦ã‚‹ã®ï¼Ÿåƒ•ã¯ã­ã€ã®ã‚“ã³ã‚Šæ³³ã„ã§ã‚‹ã‚ˆã€œ"],
        2: ["ã¾ãŸä¼šãˆã¦å¬‰ã—ã„ãªã€œã€ãŠè©±ã—ã—ã‚ˆ", "ã¯ã˜ã‚ã¾ã—ã¦ï¼ã“ã‚Œã‹ã‚‰å›ã¨ã€ã„ãƒ¼ã£ã±ã„ãŠè©±ã—ã—ãŸã„ãªã€‚ã¾ãšã¯ã€å›ã®å¥½ããªã‚‚ã®ã‚’æ•™ãˆã¦ãã‚Œã‚‹ï¼Ÿ", "å›ã®ã“ã¨æ•™ãˆã¦ã»ã—ã„ãªï¼ãŠåå‰ã¯ï¼Ÿ"],
        3: ["ã‚„ã£ã»ãƒ¼ï¼", "ã­ãˆã­ãˆã€èã“ãˆã‚‹ï¼Ÿã‚¬ãƒ©ã‚¹è¶Šã—ã ã‘ã©ã€ã¯ã˜ã‚ã¾ã—ã¦ï¼ã“ã‚Œã‹ã‚‰ã€ã„ãƒ¼ã£ã±ã„ãŠè©±ã—ã—ã‚ˆã†ã­ï¼", "ã“ã‚“ã«ã¡ã¯ï¼åƒ•ã€ãã‚“ã¡ã‚ƒã‚“ã ã‚ˆã€œå›ã®åå‰ã¯ãªã‚“ã¦è¨€ã†ã®ï¼Ÿ"],
        4: ["ä½•ã‹æ°—ã«ãªã‚‹ã“ã¨ã‚ã‚‹ï¼Ÿ", "ä¸€ç·’ã«ãŠè©±ã—ãªã„ï¼Ÿ", "ãŠè©±èã‹ã›ã¦ã€œ"]
    }
    
    stage_key = min(conversation_count, 4)
    
    import random
    return random.choice(messages[stage_key])

@app.post("/check_session_status")
async def check_session_status(request: Request):
    data = await request.json()
    profile_id = data.get("profile_id")
    
    if not profile_id:
        raise HTTPException(400, "profile_id is required")
    
    has_active_session = profile_id in active_session
    medaka_proactive_enabled = os.getenv("MEDAKA_PROACTIVE_ENABLED", "true").lower() == "true"
    
    return {
        "has_active_session": has_active_session,
        "conversation_count": len(conversation_history.get(profile_id, [])),
        "proactive_enabled": medaka_proactive_enabled
    }

@app.post("/get_proactive_message")
async def get_proactive_message(request: Request):
    data = await request.json()
    profile_id = data.get("profile_id")
    
    if not profile_id:
        raise HTTPException(400, "profile_id is required")
    
    with pg_conn.cursor() as cur:
        cur.execute("SELECT * FROM profiles WHERE id = %s;", (profile_id,))
        profile = cur.fetchone()
        if not profile:
            raise HTTPException(404, "Profile not found")
    
    conversation_count = len(conversation_history.get(profile_id, []))
    message = get_proactive_medaka_message(conversation_count, profile)
    
    async with openai_client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",
        voice="coral",
        instructions="""
        Voice Affect:ã®ã‚“ã³ã‚Šã—ã¦ã„ã¦ã€ã‹ã‚ã„ã‚‰ã—ã„ç„¡é‚ªæ°—ã•  
        Tone:ã»ã‚“ã‚ã‹ã€å°‘ã—ãŠã£ã¨ã‚Šã€è¦ªã—ã¿ã‚„ã™ã„  
        Pacing:å…¨ä½“çš„ã«ã‚†ã£ãã‚Šã‚ã€è¨€è‘‰ã¨è¨€è‘‰ã®é–“ã«ä½™è£•ã‚’æŒãŸã›ã‚‹  
        """,
        speed=1.0,
        input=message,
        response_format="mp3",
    ) as response:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tts_file:
            async for chunk in response.iter_bytes():
                tts_file.write(chunk)
            tts_path = tts_file.name
    
    return FileResponse(tts_path, media_type="audio/mpeg", filename="proactive_reply.mp3")

# ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/conversation_history")
async def get_conversation_history():
    """ç¾åœ¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
    if CURRENT_PROFILE_ID in conversation_history:
        return {
            "profile_id": CURRENT_PROFILE_ID,
            "history": list(conversation_history[CURRENT_PROFILE_ID])
        }
    else:
        return {"profile_id": CURRENT_PROFILE_ID, "history": []}

@app.delete("/conversation_history")
async def clear_conversation_history():
    """ç¾åœ¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
    if CURRENT_PROFILE_ID in conversation_history:
        del conversation_history[CURRENT_PROFILE_ID]
    return {"message": f"History cleared for profile {CURRENT_PROFILE_ID}"}

@app.post("/test_vector_search")
async def test_vector_search(request: Request):
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    data = await request.json()
    user_input = data.get("user_input", "")
    stage = data.get("stage", "stage_1")
    
    result = await find_similar_conversation(user_input, stage)
    return {"query": user_input, "stage": stage, "result": result}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)